[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cover page",
    "section": "",
    "text": "About this blog\ntry to do some interaction here later\n\n3 - Code\nThis is inline code plus a small code chunk.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndf = pd.DataFrame(data=[[1,2,3,4],\n[4,3,2,1]], columns=[\"A\",\"B\",\"C\",\"D\"],index=[\"E\", \"F\"])\n# print(df)\nplt.plot(df)\n\n\n\n\n11111\n\ndf\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\nE\n1\n2\n3\n4\n\n\nF\n4\n3\n2\n1\n\n\n\n\n\n\n\n4 - Some math stuff\n\\[\n\\int_0^1 f(x) \\ dx\n\\]"
  },
  {
    "objectID": "posts/machine_learning/06.Page Rank Algorithm.html",
    "href": "posts/machine_learning/06.Page Rank Algorithm.html",
    "title": "01 week01",
    "section": "",
    "text": "Print your full name here to replace “Hello World!” 打印姓名\n\nprint(\"Hello World!\")\n\nHello World!\n\n\nview/hide line numbers in jupyter notebook 显示/隐藏行数\nclick View -&gt; click Toggle Line Numbers 点击View -&gt; 点击Toggle Line Numbers\n\n# This is line #5   这是第5行\n\nthe order of executing block changes results jupyter notebook 中未运行的代码块对后面代码块无影响\n\nimport numpy as np\n\n\nprint(np.__version__)\n\n1.26.0\n\n\nthe order of executing block changes results jupyter notebook 代码运行顺序对结果有影响\n\na=5\n\n\nprint(a)\n\n5\n\n\n\na=6\n\nIf you run the last code block before the print block, the output will show that a is 6."
  },
  {
    "objectID": "posts/machine_learning/03.Linear and nonlinear regression 1.html",
    "href": "posts/machine_learning/03.Linear and nonlinear regression 1.html",
    "title": "Harnessing Momentum: Predictive Analysis of Stock Prices Using Linear and Nonlinear Regression I",
    "section": "",
    "text": "Intro\nMomentum strategy is a type of investment approach that aims to capitalize on the continuance of existing market trends. In this strategy, investors buy securities that are already rising and sell them when they appear to have peaked. The underlying belief is that trends can persist for some time, and it’s possible to profit_linear by staying with a trend until its conclusion.\nIn this blog, we will be focusing on two key tasks related to the momentum strategy: * Linear Regression for Stock Price Prediction: The first task involves using linear regression to predict the price of a stock. Linear regression is a statistical method that allows us to study the relationship between two continuous variables. In this case, we’ll be examining how various factors influence the price of a stock.\n\nNonlinear Regression for Price Trend Prediction: The second task will utilize nonlinear regression to predict whether the price of a stock will increase or decrease. Unlike linear regression, nonlinear regression can capture more complex relationships, making it an ideal tool for predicting the dynamic nature of stock prices.\n\n\nNecessary Packages\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nfrom sklearn.metrics import accuracy_score\n\n\n\n\nRegression models\nStarts with easy ones, we want some loss functions to determine the performances of models \\(MAE = \\frac{1}{m}\\sum_{i=1}^m\\vert y_{pred}\\,-\\,y_i\\vert\\)\n\\(MSE = \\frac{1}{m}\\sum_{i=1}^m(y_{pred}\\,-\\,y)^2\\)\n\\(RMSE = \\sqrt{\\frac{1}{m}\\sum_{i=1}^m(y_{pred}\\,-\\,y)^2}\\)\n\\(MAD = median (\\vert y_{pred}\\,-\\,median (y)\\vert)\\)\n\n\nCode\ndef my_mae(y_pred, y_test):\n  assert len(y_pred) == len(y_test)\n  m = len(y_pred)\n  res = np.abs(y_test-y_pred)\n  return res.mean()\n\ndef my_mse(y_pred, y_test):\n  assert len(y_pred) == len(y_test)\n  m = len(y_pred)\n  res = (y_test-y_pred) ** 2\n  return res.mean()\n\ndef my_rmse(y_pred, y_test):\n  assert len(y_pred) == len(y_test)\n  m = len(y_pred)\n  res = (y_test-y_pred) ** 2\n  return (res.mean()) ** .5\n\ndef my_mad(y_pred, y_test):\n  assert len(y_pred) == len(y_test)\n  m = len(y_pred)\n  res = np.abs(y_pred-np.median(y_test))\n  return np.median(res)\n\n\n\n\n\nLinear Regression\n\\(f(x_i) = W^Tx\\,+\\,b~~~→~~~f(x)=w_1x_1+\\dots+w_nx_n+b\\)\nThe Formula method:\n\\(\\beta = (X^TX)^{-1}X^TY\\)\nWhere \\(\\beta\\) is: \\[\n\\begin{matrix}\nw_1 \\\\\nw_2 \\\\\n\\vdots \\\\\nb   \\\\\n\\end{matrix}\n\\]\nDefine the function based on the formula:\n\n\nCode\ndef formulaMethod(x, y):\n  ones = np.array([x.shape[0] * [1]]).T\n  x = np.hstack((x, ones))\n  xMat = np.mat(x)\n  yMat = np.mat(y).T\n  xTx = np.dot(xMat.T, xMat)\n  try:\n    assert np.linalg.det(xTx) != 0\n  except:\n    print(f\"xTx is {xTx} \\n which is unable to inverse\")\n  ws = np.dot(xTx.I, np.dot(xMat.T, yMat))\n  return ws\n\ndef linearPredict(x, ws):\n  # assume x has m (number of observations) rows\n  # and n (number of features) columns\n  # assume ws has (n+1) weights, means weights and a constent\n  ones = np.ones([x.shape[0], 1])\n  x_1 = np.hstack((x, ones))\n  y_pred = np.dot(x_1,ws)\n  return y_pred\n\n\nTest the Formula function:\n\n\nCode\nx1 = np.array([9,8,7,6,5])\nx2 = np.array([2,3,4,5,6])\nx3 = np.array([1,3,5,7,9])\ny = 2*x1 + 7*x2 - 9*x3 + 18\ndata = np.array([x1, x2, x3, y]).T\n# print(data)\nx = data[:,:-1]\nws = formulaMethod(x, y)\n\ny_pred = linearPredict(x, ws)\nrmse = my_rmse(y_pred, y)\nprint(f\"The loss is: {rmse:.2f}\")\nplt.figure()\nplt.title(\"Prediced y VS. Actual y\", fontname='serif', color='darkblue', fontsize=16)\nplt.plot(y, label=r'$y = 2x_1 + 7x_2 - 9x_3 + 18$')\nplt.plot(y_pred, 'm--', label='prediced y')\nplt.xlabel(\"Observations\",  fontname='serif', color='darkred')\nplt.ylabel(\"Values\",  fontname='serif', color='darkred')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nThe loss is: nan\n\n\n\n\n\nThe formular method has several limitations. It requires the matrix \\(X^TX\\) has inverse, and the plot shows it could not fit_linear well. Therefore, I would prefer the approximation method.\nThe method of Approximation:\n\\(\\theta_0 = 2X^T(X\\beta - Y)\\)\n\\(\\theta_{k+1} = \\theta_k - \\alpha\\cdot2X^T(X\\beta - Y)\\)\nWhere \\(\\alpha\\) is the learning rate\nDefine and test the update weight function:\n\n\nCode\ndef linear_update_weigths(x, y, w, lr):\n  yMat = np.mat(y).T\n  y_pred = np.dot(x,w)\n  gradient = np.array(lr * 2 * np.dot(x.T, (y_pred - y).T)).flatten()\n  w -= gradient\n  y_pred = np.dot(x,w)\n  return w, y_pred\n\ndef linear_update_weigths_1(x, y, w, lr, func=my_rmse):\n  # another method that also works\n  # y_pred = linearPredict(x[:,:-1], w)\n  y_pred = np.dot(x,w)\n  # loss_arr = (y_pred - y)\n  loss_arr = np.array([func(y_pred, y)] * len(y))\n  gradient = np.array([np.dot(x.T, loss_arr) / x.shape[0]]).T.flatten()\n  w = w - lr * gradient\n  y_pred = np.dot(x,w)\n  return w, y_pred\n\nones = np.ones([x.shape[0], 1])\nx_1 = np.hstack((x, ones))\nws = np.random.rand(x_1.shape[1])  # initialize weight\n\nw, y_pred = linear_update_weigths(x_1, y, ws, 0.001)\nloss = my_rmse(y, y_pred)\nprint(f\"loss in this test linear regression update: {loss:.2f}\")\n\nw, y_pred = linear_update_weigths_1(x_1, y, ws, 0.001)\nloss = my_rmse(y, y_pred)\nprint(f\"loss in this test linear regression update_1: {loss:.2f}\")\n\n\nloss in this test linear regression update: 18.29\nloss in this test linear regression update_1: 18.39\n\n\nPower Regression\n\\(y = \\sum_{i=1}^mw_jx^j + b\\)\nTreat \\(x^n\\) as \\(x_n\\) in linear regression.\nDefine the constructor of matrix based on x array and power index:\n\n\nCode\ndef to_power(x, power):\n  x = x.reshape(x.shape[0], -1)\n  x_tmp = x[:]\n  for i in range(2, power+1):\n      x = np.append(x, x_tmp ** i, axis=1)\n  return x\n\nx = np.linspace(0, 2, 10)\n\ny = 7*x - 11*(x**2) + 13*(x**5) - 27\nx = to_power(x, 5)\n\nones = np.ones([x.shape[0], 1])\nx_1 = np.hstack((x, ones))\nws = np.random.rand(x_1.shape[1])  # initialize weight\n\nw, y_pred = linear_update_weigths(x_1, y, ws, 0.001)\nloss = my_rmse(y, y_pred)\nprint(f\"loss in this test power regression update: {loss:.2f}\")\nw, y_pred = linear_update_weigths_1(x_1, y, ws, 0.001)\nloss = my_rmse(y, y_pred)\nprint(f\"loss in this test power regression update_1: {loss:.2f}\")\n\n\nloss in this test power regression update: 332.84\nloss in this test power regression update_1: 295.04\n\n\n\nTrain and Evaluate Approximation method\nLinear regression\n\n\nCode\ndef fit_linear(x, y, lr=0.001, num_iter=2000, func=my_rmse):\n  perform_df = pd.DataFrame(columns=['loss'])\n  ones = np.ones([x.shape[0]]).reshape(-1,1)\n  x_1 = np.hstack((x, ones))\n  weight = np.random.rand(x_1.shape[1])  # initialize weight\n  for epoch in range(num_iter):\n    weight, y_pred = linear_update_weigths(x_1, y, weight, lr)\n    # weight, y_pred = linear_update_weigths_1(x_1, y, weight, lr, func)\n    loss = func(y, y_pred)\n    perform_df.loc[epoch] = [loss]\n  return weight, y_pred, perform_df\n\n\n### x and y\nn = 100  # number of rows\nx1 = np.linspace(0, 2, n)\nx2 = np.linspace(-2, 10, n)\nx3 = np.linspace(-5, 7, n)\n\ny = 3*x1 - 7*x2 + 6*x3 + 11\nx = np.array([x1, x2, x3]).T\n\n### training hyper-parameter\nlearning_rate = 0.0001\niterations = 1000\nweight, y_pred, perform_df = fit_linear(x, y, lr=learning_rate, num_iter=iterations)\n\nloss = my_rmse(y, y_pred)\nprint(f\"Final loss:{loss:.2f}\")\n\nfig = plt.figure()\n# plt.suptitle(\"Prediction in Linear regression\")\n\nleft, bottom, width, height = 0, 0, 1, 1\nax1 = fig.add_axes([left, bottom, width, height])\nax1.plot(y, label=r'$y = 3x_1 - 7x_2 + 6x_3 + 11$')\nax1.plot(y_pred, 'm--', label='prediced y')\nax1.set_xlabel(\"Observations\", fontname='serif', color='darkred')\nax1.set_ylabel(\"Values\", fontname='serif', color='darkred')\nax1.set_title(\"Prediced y VS. Actual y\", fontname='serif', color='darkblue', fontsize=16)\nax1.legend()\n\nleft, bottom, width, height = 0.6, 0.5, 0.3, 0.3\nax2 = fig.add_axes([left, bottom, width, height])\nax2.plot(perform_df, label=r'loss', color='#51c4d3')\nax2.set_xlabel(\"Epoch\", fontname='serif', color='darkred')\nax2.set_ylabel(\"Loss\", fontname='serif', color='darkred')\nax2.set_title(\"Loss y VS. Epoch\", fontname='serif', color='darkblue', fontsize=16)\nax2.legend()\nax2.grid()\nplt.tight_layout()\nplt.show()\n\n\nFinal loss:0.00\n\n\n\n\n\nPower regression\n\n\nCode\nn = 100\nx = np.linspace(-2, 2, n)\n\ny = -7*x + 11*(x**2) - 1*(x**3) - 27\nx = to_power(x, 3)\n\nlearning_rate = 0.0001\niterations = 2000\n\nweight, y_pred, perform_df = fit_linear(x, y, lr=learning_rate, num_iter=iterations)\n\nloss = my_rmse(y, y_pred)\nprint(f\"Final loss:{loss:.2f}\")\nfig = plt.figure()\n# plt.suptitle(\"Prediction in Power regression\")\n\nleft, bottom, width, height = 0, 0, 1, 1\nax1 = fig.add_axes([left, bottom, width, height])\nax1.plot(y, label=r'$y = -7x + 11x^2 - x^3 - 27$')\nax1.plot(y_pred, 'm--', label='prediced y')\nax1.set_xlabel(\"Observations\", fontname='serif', color='darkred')\nax1.set_ylabel(\"Values\", fontname='serif', color='darkred')\nax1.set_title(\"Prediced y VS. Actual y\", fontname='serif', color='darkblue', fontsize=16)\nax1.legend()\n\nleft, bottom, width, height = 0.6, 0.5, 0.3, 0.3\nax2 = fig.add_axes([left, bottom, width, height])\nax2.plot(perform_df, label=r'loss', color='#51c4d3')\nax2.set_xlabel(\"Epoch\", fontname='serif', color='darkred')\nax2.set_ylabel(\"Loss\", fontname='serif', color='darkred')\nax2.set_title(\"Loss y VS. Epoch\", fontname='serif', color='darkblue', fontsize=16)\nax2.legend()\nax2.grid()\nplt.tight_layout()\nplt.show()\n\n\nFinal loss:0.00\n\n\n\n\n\n\n\n\nLogistic(NonLinear) Regression\n\nSigmoid Function: \\(\\sigma(x) = \\frac{1}{1+e^{-x}}\\)\nSigmoid Functio maps any real value into another value between 0 and 1, it is used to model the probability that the target variable belongs to a particular category.\nLoss Function: \\(J(\\theta) = -\\frac{1}{m} \\sum^{m}_{i=1}[y^i log(h_{\\theta}(x^i)) + (1-y^i)log(1-h_{\\theta}(x^i)]\\)\nwhere \\(h_{\\theta}​(x)\\) is the predicted value.\nIn logistic regression, it’s generally better use log loss.\nGradient: \\(\\triangledown J(\\theta) = \\frac{1}{m}X^T(h-y)\\)\n\nWeight Update Rule: \\(\\theta = \\theta - \\alpha\\triangledown J(\\theta)\\)\n\n\n\nCode\ndef sigmoid(x):\n  return 1 / (1 + np.exp(-x))\n\ndef logLoss(y, h):\n  return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n\ndef normalize_and_binarize(y):\n  # y_normalized = (y - np.min(y)) / (np.max(y) - np.min(y))\n  # y_binarized = np.where(y_standardized &gt;= 0.5, 1, 0)\n  y_standardized = (y - np.mean(y)) / np.std(y)\n  y_binarized = np.where(y_standardized &gt;= 0, 1, 0)\n  return y_binarized\n\ndef logistic_update_weigths(x, y, w, lr):\n  h = sigmoid(np.dot(x, w))\n  loss = logLoss(y, h)\n  gradient = np.dot(x.T, (h - y)) / y.shape[0]\n  w -= lr *gradient\n  h = sigmoid(np.dot(x, w))\n  return w, h\n\n### x and y\nn = 100  # number of rows\nx1 = np.linspace(0, 2, n)\nx2 = np.linspace(-2, 10, n)\nx3 = np.linspace(-5, 7, n)\n\nx = np.array([x1, x2, x3]).T\n\ny = 2*x1 + 7*x2 - 9*x3 + 18\ny = normalize_and_binarize(y)\n\nones = np.ones([x.shape[0], 1])\nx_1 = np.hstack((x, ones))\nweight = np.random.rand(x_1.shape[1])  # initialize weight\n\nweight, h = logistic_update_weigths(x_1, y, weight, 0.001)\ny_pred = normalize_and_binarize(h)\nloss = logLoss(y, h)\naccu = accuracy_score(y, y_pred)\nprint(f\"loss, accuray in this test logistic regression update: loss:{loss:.2f},accuracy:{accu:.2f}\")\n\n\nloss, accuray in this test logistic regression update: loss:2.46,accuracy:0.16\n\n\n\nTrain and Evaluate Logistic Regression\n\n\nCode\ndef fit_logistic(x, y, lr=0.001, num_iter=2000):\n  perform_df = pd.DataFrame(columns=['loss'])\n  ones = np.ones([x.shape[0], 1])\n  x_1 = np.hstack((x, ones))\n  weight = np.random.rand(x_1.shape[1])  # initialize weight\n  for epoch in range(num_iter):\n    weight, h = logistic_update_weigths(x_1, y, weight, lr)\n    loss = logLoss(y, h)\n    perform_df.loc[epoch] = [loss]\n  return weight, h, perform_df\n\n### x and y\nn = 100  # number of rows\nx1 = np.linspace(0, 2, n)\nx2 = np.linspace(-2, 10, n)\nx3 = np.linspace(-5, 7, n)\n\ny = 3*x1 - 7*x2 + 6*x3 + 11\ny = normalize_and_binarize(y)\nx = np.array([x1, x2, x3]).T\n\n### hyper-parameters\nlearning_rate = 0.001\niterations = 1000\nweight, h, perform_df = fit_logistic(x, y, lr=learning_rate, num_iter=iterations)\ny_pred = normalize_and_binarize(h)\n\nloss = my_rmse(y, h)\naccu = accuracy_score(y, y_pred)\nprint(f\"Final loss:{loss:.2f}, accuracy: {accu:.2f}\")\n\n\nfig = plt.figure()\n# plt.suptitle(\"Prediction in Logistic regression\")\n\nleft, bottom, width, height = 0, 0, 1, 1\nax1 = fig.add_axes([left, bottom, width, height])\nax1.plot(y, label=r'$y = [3x_1 - 7x_2 + 6x_3 + 11] &gt; -8$')\nax1.plot(y_pred, 'm--', label='prediced y')\nax1.set_xlabel(\"Observations\")\nax1.set_ylabel(\"Values\")\nax1.set_title(\"Prediced y VS. Actual y\", fontname='serif', color='darkblue', fontsize=16)\nax1.legend()\n\nleft, bottom, width, height = 0.6, 0.5, 0.3, 0.3\nax2 = fig.add_axes([left, bottom, width, height])\nax2.plot(perform_df, label=r'loss', color='#51c4d3')\nax2.set_xlabel(\"Epoch\", fontname='serif', color='darkred',)\nax2.set_ylabel(\"Loss\", fontname='serif', color='darkred',)\nax2.set_title(\"Loss y VS. Epoch\", fontname='serif', color='darkblue', fontsize=16)\nax2.legend()\nax2.grid()\nplt.tight_layout()\nplt.show()\n\n\nFinal loss:0.31, accuracy: 0.94\n\n\n\n\n\n\n\nPlayground\nadd interaction later I tried python shiny and ipywidgets.interact, both don’t work\n#| standalone: true\n\nfrom shiny import *\n\napp_ui = ui.page_fluid(\n    ui.input_slider(\"n\", \"N\", 0, 100, 40),\n    ui.output_text_verbatim(\"txt\"),\n)\n\ndef server(input, output, session):\n    @output\n    @render.text\n    def txt():\n        return f\"The value of n*2 is {input.n() * 2}\"\n\napp = App(app_ui, server)\n\ncontinue in next blog.\nnext\ngo back"
  },
  {
    "objectID": "posts/machine_learning/02.Clustering.html",
    "href": "posts/machine_learning/02.Clustering.html",
    "title": "Clustering on Brain Networks Observations",
    "section": "",
    "text": "Intro\nThis project is a practical exploration into the workings of clustering algorithms. The brain_networks dataset from seaborn is used as a basis for experimentation. A highlight of the project is the implementation of the DBSCAN algorithm from scratch, providing a deeper understanding of its inner workings. Other models from scikit-learn are also utilized to observe the unique characteristics and results each model produces.\n\n\nNecessary Packages\n\n\nCode\nimport re\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nplt.style.use('ggplot')\nprint(f\"List of seaborn datasets: \\n{sns.get_dataset_names()}\")\n\n\nList of seaborn datasets: \n['anagrams', 'anscombe', 'attention', 'brain_networks', 'car_crashes', 'diamonds', 'dots', 'dowjones', 'exercise', 'flights', 'fmri', 'geyser', 'glue', 'healthexp', 'iris', 'mpg', 'penguins', 'planets', 'seaice', 'taxis', 'tips', 'titanic']\n\n\n\n\nData Process\n\nDownload Data\n\n\nCode\nbrain_networks = sns.load_dataset('brain_networks', header=[0, 1, 2], index_col=0)\nprint(f\"There are {brain_networks.isna().sum().sum()} missing values\")\nbrain_networks\n\n\nThere are 0 missing values\n\n\n\n\n\n\n\n\nnetwork\n1\n2\n3\n4\n5\n...\n16\n17\n\n\nnode\n1\n1\n1\n1\n1\n...\n3\n4\n1\n2\n3\n4\n\n\nhemi\nlh\nrh\nlh\nrh\nlh\nrh\nlh\nrh\nlh\nrh\n...\nrh\nlh\nrh\nlh\nrh\nlh\nrh\nlh\nrh\nlh\n\n\n\n\n0\n56.055744\n92.031036\n3.391576\n38.659683\n26.203819\n-49.715569\n47.461037\n26.746613\n-35.898861\n-1.889181\n...\n0.607904\n-70.270546\n77.365776\n-21.734550\n1.028253\n7.791784\n68.903725\n-10.520872\n120.490463\n-39.686432\n\n\n1\n55.547253\n43.690075\n-65.495987\n-13.974523\n-28.274963\n-39.050129\n-1.210660\n-19.012897\n19.568010\n15.902983\n...\n57.495071\n-76.393219\n127.261360\n-13.035799\n46.381824\n-15.752450\n31.000332\n-39.607521\n24.764011\n-36.771008\n\n\n2\n60.997768\n63.438793\n-51.108582\n-13.561346\n-18.842947\n-1.214659\n-65.575806\n-85.777428\n19.247454\n37.209419\n...\n28.317369\n9.063977\n45.493263\n26.033442\n34.212200\n1.326110\n-22.580757\n12.985169\n-75.027451\n6.434262\n\n\n3\n18.514868\n12.657158\n-34.576603\n-32.665958\n-7.420454\n17.119448\n-41.800869\n-58.610184\n32.896915\n11.199619\n...\n71.439629\n65.842979\n-10.697547\n55.297466\n4.255006\n-2.420144\n12.098393\n-15.819172\n-37.361431\n-4.650954\n\n\n4\n-2.527392\n-63.104668\n-13.814151\n-15.837989\n-45.216927\n3.483550\n-62.613335\n-49.076508\n18.396759\n3.219077\n...\n95.597565\n50.960453\n-23.197300\n43.067562\n52.219875\n28.232882\n-11.719750\n5.453649\n5.169828\n87.809135\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n915\n-7.429513\n-4.813219\n-3.670537\n1.442261\n-19.680431\n-29.109356\n-3.376562\n-8.764893\n0.017912\n-25.305094\n...\n58.285793\n68.557411\n22.833048\n76.179489\n51.934669\n-6.614513\n-6.690762\n22.893030\n48.274380\n76.228455\n\n\n916\n-33.554138\n-38.605621\n-25.664803\n-30.252352\n5.562785\n20.182186\n17.911247\n24.653582\n-32.935612\n-21.783203\n...\n59.864819\n23.004578\n7.657463\n50.962399\n13.696922\n63.503616\n57.401176\n24.974548\n51.972153\n64.538788\n\n\n917\n-78.539566\n-74.197189\n-54.041595\n-39.970291\n-14.277059\n-30.606461\n60.628521\n62.027023\n-32.800556\n-37.021500\n...\n68.767868\n49.664017\n64.551498\n43.800747\n8.144480\n47.281460\n70.499649\n66.994400\n81.539246\n64.969772\n\n\n918\n-103.235825\n-98.744286\n-40.109543\n-44.907734\n12.109148\n1.621340\n33.765560\n55.356071\n-14.330512\n-17.224781\n...\n103.155251\n106.454849\n9.046827\n46.674419\n40.954796\n0.877180\n37.577152\n20.517746\n3.124434\n56.718388\n\n\n919\n-36.288868\n-10.762070\n-30.356262\n-23.319504\n14.252188\n-27.559860\n17.279512\n-19.060152\n26.558777\n21.377319\n...\n102.086304\n80.051140\n-2.642610\n-12.229620\n-6.596726\n17.665163\n16.153173\n8.300399\n33.687531\n17.960655\n\n\n\n\n920 rows × 62 columns\n\n\n\nIt seems this dateset has 3 columns, we need to merge the columns.\n\n\nCode\nbrain_networks_merge = brain_networks.copy()\nbrain_networks_merge.columns = brain_networks_merge.columns.map(lambda x: '_'.join(str(i) for i in x))\nbrain_networks_merge.head()\n\n\n\n\n\n\n\n\n\n1_1_lh\n1_1_rh\n2_1_lh\n2_1_rh\n3_1_lh\n3_1_rh\n4_1_lh\n4_1_rh\n5_1_lh\n5_1_rh\n...\n16_3_rh\n16_4_lh\n16_4_rh\n17_1_lh\n17_1_rh\n17_2_lh\n17_2_rh\n17_3_lh\n17_3_rh\n17_4_lh\n\n\n\n\n0\n56.055744\n92.031036\n3.391576\n38.659683\n26.203819\n-49.715569\n47.461037\n26.746613\n-35.898861\n-1.889181\n...\n0.607904\n-70.270546\n77.365776\n-21.734550\n1.028253\n7.791784\n68.903725\n-10.520872\n120.490463\n-39.686432\n\n\n1\n55.547253\n43.690075\n-65.495987\n-13.974523\n-28.274963\n-39.050129\n-1.210660\n-19.012897\n19.568010\n15.902983\n...\n57.495071\n-76.393219\n127.261360\n-13.035799\n46.381824\n-15.752450\n31.000332\n-39.607521\n24.764011\n-36.771008\n\n\n2\n60.997768\n63.438793\n-51.108582\n-13.561346\n-18.842947\n-1.214659\n-65.575806\n-85.777428\n19.247454\n37.209419\n...\n28.317369\n9.063977\n45.493263\n26.033442\n34.212200\n1.326110\n-22.580757\n12.985169\n-75.027451\n6.434262\n\n\n3\n18.514868\n12.657158\n-34.576603\n-32.665958\n-7.420454\n17.119448\n-41.800869\n-58.610184\n32.896915\n11.199619\n...\n71.439629\n65.842979\n-10.697547\n55.297466\n4.255006\n-2.420144\n12.098393\n-15.819172\n-37.361431\n-4.650954\n\n\n4\n-2.527392\n-63.104668\n-13.814151\n-15.837989\n-45.216927\n3.483550\n-62.613335\n-49.076508\n18.396759\n3.219077\n...\n95.597565\n50.960453\n-23.197300\n43.067562\n52.219875\n28.232882\n-11.719750\n5.453649\n5.169828\n87.809135\n\n\n\n\n5 rows × 62 columns\n\n\n\n\n\nCode\nfig = plt.figure(figsize=(18, 72))\nfor network in range(1, 18):\n  network_columns = [col for col in brain_networks_merge.columns if re.match(f'^{network}_', col)]\n  nodes_len = len(set(int(re.search('_([0-9]+)_', col).group(1)) for col in network_columns))\n  for node in range(1, nodes_len+1):\n\n    index = (network - 1) * nodes_len + node\n    plt.subplot(17, nodes_len, index)\n    data_filter = brain_networks_merge.filter(regex=f'^{network}_{node}_', axis=1)\n    \n    sns.scatterplot(data=data_filter)\n\n# Set the super title\nplt.suptitle('Brain Networks Data', fontname='serif', color='darkblue', fontsize=20, y=0.885)\n\n# Set the super x and y labels\nfig.text(0.5, 0.105, 'Node', ha='center', va='center', fontsize=16, fontname='serif', color='darkred')\nfig.text(0.08, 0.5, 'Network', ha='center', va='center', rotation='vertical', fontsize=16, fontname='serif', color='darkred')\n\n# plt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nDBSCAN\n\n\nCode\ndef euclidean_distance(a, b):\n    return np.sqrt(np.sum((a - b)**2))\n\ndef region_query(data, point_id, eps):\n    n_points = data.shape[1]\n    seeds = []\n    for i in range(0, n_points):\n        if euclidean_distance(data[:, point_id], data[:, i]) &lt; eps:\n            seeds.append(i)\n    return seeds\n\ndef expand_cluster(data, classifications, point_id, cluster_id, eps, min_points):\n    seeds = region_query(data, point_id, eps)\n    if len(seeds) &lt; min_points:\n        classifications[point_id] = -1\n        return False\n    else:\n        classifications[point_id] = cluster_id\n        for seed_id in seeds:\n            classifications[seed_id] = cluster_id\n\n        while len(seeds) &gt; 0:\n            current_point = seeds[0]\n            results = region_query(data, current_point, eps)\n            if len(results) &gt;= min_points:\n                for i in range(0, len(results)):\n                    result_point = results[i]\n                    if classifications[result_point] == 0 or classifications[result_point] == -1:\n                        if classifications[result_point] == 0:\n                            seeds.append(result_point)\n                        classifications[result_point] = cluster_id\n            seeds = seeds[1:]\n        return True\n\ndef dbscan(data, eps, min_points):\n    cluster_id = 1\n    n_points = data.shape[1]\n    classifications = [0]*n_points\n    for point_id in range(0, n_points):\n        point = data[:, point_id]\n        if classifications[point_id] == 0:\n            if expand_cluster(data, classifications, point_id, cluster_id, eps, min_points):\n                cluster_id = cluster_id + 1\n    return classifications\n\n\n\n\nCode\ndata = brain_networks_merge\ndata = brain_networks_merge.filter(regex=f'^{17}_{4}_', axis=1)\ndata = brain_networks_merge.values.T\n\nscaler = StandardScaler()\ndata = scaler.fit_transform(data)\n\neps = 7\nmin_points = 8\n\ndbscan_labels = dbscan(data, eps, min_points)\ndbscan_labels = [label + 1 for label in dbscan_labels]\n\ncounts = np.bincount(dbscan_labels)\n\nfor i in range(len(counts)):\n    if i == 0:\n        print(f\"Noise points: {counts[i]}\")\n    else:\n        print(f\"Cluster {i}: {counts[i]} points\")\n\n\nNoise points: 468\nCluster 1: 0 points\nCluster 2: 452 points\n\n\nTry other functions from sklearn\n\n\nCode\nfrom sklearn.cluster import KMeans, SpectralClustering, AgglomerativeClustering, OPTICS\nfrom sklearn.mixture import GaussianMixture\n\ndata = brain_networks_merge\n\n# K-Means\nkmeans = KMeans(n_clusters=3, init='k-means++', n_init=10)\nkmeans_labels = kmeans.fit_predict(data)\n\n# Spectral Clustering\nspectral = SpectralClustering(n_clusters=3, eigen_solver='arpack', n_init=10)\nspectral_labels = spectral.fit_predict(data)\n\n# Agglomerative Clustering\nagglo = AgglomerativeClustering(n_clusters=3, metric='euclidean', linkage='ward')\nagglo_labels = agglo.fit_predict(data)\n\n# OPTICS\noptics = OPTICS(min_samples=5, xi=0.05, min_cluster_size=0.05)\noptics_labels = optics.fit_predict(data)\n\n# Gaussian Mixture Models\ngmm = GaussianMixture(n_components=3, covariance_type='full', tol=0.001)\ngmm_labels = gmm.fit_predict(data)\n\n\n\n\nCode\nmodel_name = ['DBSCAN', 'K-means', 'Spectral Clustering', 'Agglomerative Clustering', 'OPTICS', 'Gaussian Mixture Models']\nmodel_labels = [dbscan_labels, kmeans_labels, spectral_labels, agglo_labels, optics_labels, gmm_labels]\n\nplt.figure(figsize=(15,10))\n\nfor i, e in enumerate(model_name):\n  labels = model_labels[i]\n  plt.subplot(3, 2, i+1)\n  df = brain_networks_merge.copy()\n  df[e] = labels \n  plt.subplot(3, 2, i+1)\n  sns.scatterplot(data=df, x=df.index, y=df.columns[0], hue=e, palette='viridis')\n  plt.xlabel('Index', fontname='serif', color='darkred')\n  plt.ylabel('Value', fontname='serif', color='darkred')\n  plt.title(f\"{e} Result\", fontname='serif', color='darkblue')\n\n\nplt.suptitle('Clustering Results', fontname='serif', color='blue', fontsize=16, y=1.0)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nWe can have a violin plot for more clear view of how many observations in each cluster by each model\n\n\nCode\ndf_labels = pd.DataFrame()\n\ndf_labels['DBSCAN'] = dbscan_labels\ndf_labels['K-means'] = kmeans_labels\ndf_labels['Spectral Clustering'] = spectral_labels\ndf_labels['Agglomerative Clustering'] = agglo_labels\ndf_labels['OPTICS'] = optics_labels\ndf_labels['Gaussian Mixture Models'] = gmm_labels\n\ndf_melt = df_labels.melt(var_name='model', value_name='label')\n\nplt.figure(figsize=(12,6))\nsns.violinplot(x='model', y='label', hue='model', data=df_melt, palette='viridis', legend=False)\nplt.title('Clustering Results', fontname='serif', color='darkblue', fontsize=16)\nplt.xlabel('Model', fontname='serif', color='darkred')\nplt.ylabel('Label', fontname='serif', color='darkred')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nend.\ngo back"
  },
  {
    "objectID": "posts/machine_learning/machine_learning.html",
    "href": "posts/machine_learning/machine_learning.html",
    "title": "Series: CS5805 Machine Learning",
    "section": "",
    "text": "About this series\nCourse projects for CS5805\n\n5 blog posts are required, not 3\nTopics are pre-defined, not up to you to decide:\n\nProbability theory and random variables\nClustering\nLinear and nonlinear regression\nClassification\nAnomaly/outlier detection\n\nThe requirements for Machine Learning code and at least one data visualization are still in effect.\nAlso unchanged is my expectation that everyone will build their own blog, preferably using Quarto, and publish it online, preferably on GitHub Pages. You can build the site with something else, like a JavaScript framework, and publish it elsewhere: https://quarto.org/docs/websites/website-blog.html#publishing,Links to an external site. as long as the blog is built from source files programmatically (not using a GUI) and it is accessible for free via a URL.\n\n\n\n\n\n\n\n\n\n\n\nPredicting Diamond Prices: A Naive Bayes Approach\n\n\n\n\n\nThis project applies Probability and the Naive Bayes algorithm to predict diamond prices using the categorical features of the Seaborn’s diamonds dataset, demonstrating the algorithm’s efficiency in handling such data.\n\n\n\n\n\n\nNov 16, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\nClustering on Brain Networks Observations\n\n\n\n\n\nThis project applies clustering algorithms on seaborn’s brain_networks dataset, notably implementing DBSCAN from scratch and comparing results from various scikit-learn models.\n\n\n\n\n\n\nNov 16, 2023\n\n\n6 min\n\n\n\n\n\n\n\n\nHarnessing Momentum: Predictive Analysis of Stock Prices Using Linear and Nonlinear Regression I\n\n\n\n\n\nImplement my own version of linear regression and logistic regression and construct a momentum strategy based on my models in stock market. Phrase I, model implementation\n\n\n\n\n\n\nNov 16, 2023\n\n\n9 min\n\n\n\n\n\n\n\n\nHarnessing Momentum: Predictive Analysis of Stock Prices Using Linear and Nonlinear Regression II\n\n\n\n\n\nImplement my own version of linear regression and logistic regression and construct a time series momentum strategy based on my models in stock market. Phrase II, momentum strategy\n\n\n\n\n\n\nNov 16, 2023\n\n\n7 min\n\n\n\n\n\n\n\n\n01 week01\n\n\n\n\n\nweek01 class materials, simple jupyter notebook tips\n\n\n\n\n\n\nNov 16, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nOutlier Detection in Dow Jones using Support Vector Machines\n\n\n\n\n\nExplore the application of SVM for anomaly detection in the Dow Jones data.\n\n\n\n\n\n\nNov 16, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n01 week01\n\n\n\n\n\nweek01 class materials, simple jupyter notebook tips\n\n\n\n\n\n\nOct 6, 2023\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/intro_to_python/week11-renpy.html",
    "href": "posts/intro_to_python/week11-renpy.html",
    "title": "11 Ren’py",
    "section": "",
    "text": "script.rpy in Ren’py\nNotice that at the time this course was teaching, the lastest version of Ren’py is 7.4, which is the last version using Python 2.7. Then, they shifted to Ren’py 8.0, which using Python 3.\n#| code-fold: true #| eval: false # 游戏的脚本可置于此文件中。 init python: # init python 语句在初始化阶段运行，早于其他游戏资源加载。 pass # import numpy as np\n\n\n声明此游戏使用的角色。颜色参数可使角色姓名着色。\ndefine author = Character(“张晴森”, color=‘#7955d1’) define abc = Character(“abc123”)\n\n\n游戏在此开始。\nlabel start:\n# 显示一个背景。此处默认显示占位图，但您也可以在图片目录添加一个文件\n# （命名为“bg room.png”或“bg room.jpg”）来显示。\n\"在script中仅有字符串时为无角色对话\"\n\nabc \"上面定义了角色abc123，角色名加字符串为简单对话\"\nauthor \"上面定义了角色{b}{i}{size=45}{alpha=0.7}{color=#7955d1}张晴森{/color}{/alpha}{/size}{/i}{/b}，角色名显示为所设置的颜色\"\n\n\"文字标签\"\n# https://www.renpy.cn/doc/text.html\n\"用 \\{size\\} 标签设置文字大小\"\n\"如{size=45}abc123{/size}和{size=15}abc123{/size}\"\n\"用 \\{color\\} 标签设置文字颜色\"\n\"如{color=#7955d1}abc123{/color}和{color=#86aa2e}abc123{/color}\"\n\"用 \\{alpha\\} 标签设置文字透明度\"\n\"如{alpha=0.5}abc123{/alpha}\"\n\"用 \\{s\\} 标签设置文字粗体\"\n\"如{s}删除线{/s}\"\n\"用 \\{b\\} 标签设置文字粗体\"\n\"如{b}bold{/b}\"\n\"用 \\{i\\} 标签设置文字斜体\"\n\"如{i}italic{/i}\"\n\"结合使用\"\n\"如{b}{size=45}{alpha=0.7}{color=#7955d1}abc123{/color}{/alpha}{/size}{/b}和{i}{size=15}{alpha=0.3}{color=#86aa2e}abc123{/alpha}{/size}{/i}\"\n\n\"显示背景\"\nscene bg win\n\"使用scence显示背景，bg win可以是'bg win.jpg'或者'bg win.png'或者其他图像格式\"\n\n# 显示角色立绘。此处使用了占位图，但您也可以在图片目录添加命名为\n# “eileen happy.png”的文件来将其替换掉。\n\"显示人物图像\"\nshow eileen happy\n\"{i}show eileen happy{/i}\"\n\"未提前定义的图片也可直接通过'show'引用\"\n\"RenPy 会自动去 Images 文件夹搜索 'eileen happy'.jpg/.png等\"\n\n# 此处显示各行对话。\ndefine e = Character(\"艾琳\", image=\"eileen\", color='#e3b4b8')\nimage eileen concerned = \"character/eileen concerned.png\"           # 路径为: '.\\images\\character\\xxx.jpg' images省略，默认从images搜索图片\nimage side eileen concerned = \"character/side eileen concerned.png\" # side设置头像\nimage eileen happy = \"character/eileen happy.png\"\nimage side eileen happy = \"character/side eileen happy.png\"\nimage eileen vhappy = \"character/eileen vhappy.png\"\nimage side eileen vhappy = \"character/side eileen vhappy.png\"\n\ne \"定义角色\\{艾琳\\}，同时定义其图片\"\ne concerned \"艾琳 {b}concerned{/b}\"\ne happy \"艾琳 {b}happy{/b}\"\ne vhappy \"艾琳 {b}very happy{/b}\"\ne \"可以看到头像随人物图像的变动而变动\"\n\ne \"语音\"\nvoice \"audio/D12_945.wav\"\ne \"此外，鲨鱼，鱼龙和海豚仅管外貌上很相似，都擅长游泳，有人把它们归属鱼类\"\ne \"具体参考 https://www.renpy.cn/doc/voice.html\"\n\nhide eileen\n\"使用hide来隐藏图像，达到角色退场的效果\"\n\"注意是hide图像eileen，而不是hide角色e\"\ne concerned \"角色退场后说话\"\nshow eileen happy\ne \"重新展示角色\"\n\nscene bg blank with fade\n# region 转场特效\n    # None 无转场特效\n    # fade          0.5秒时间画面逐渐暗淡至全黑，然后0.5秒时间画面从全黑逐渐变亮成新界面。\n    # dissolve      0.5秒时间，使用溶解效果从旧界面切到新界面。\n    # pixellate     0.5秒像素化旧场景，并0.5秒反向像素化至新场景。\n    # move          通过在图像上移动位置切换场景。\n    # moveinright   从界面上对应的边界移入图像，用时0.5秒。(moveinleft, moveintop, moveinbottom)\n    # moveoutright  从界面上对应的边界移出图像，用时0.5秒。(moveoutleft, moveouttop, moveoutbottom)\n    # ease          类似于上面的move系列转场效果，差别在于ease系列基于余弦曲线放缓开始和结束的转场。(easeinright, easeinleft, easeintop, easeinbottom, easeoutright, easeoutleft, easeouttop, easeoutbottom)\n    # zoomin        镜头放大切入图像，耗时0.5秒。\n    # zoomout       镜头缩小离开图像，耗时0.5秒。\n    # zoominout     先镜头放大切入图像，然后镜头缩小离开图像，耗时0.5秒。\n    # vpunch        这种转场效果，会垂直摇晃界面0.25秒。\n    # hpunch        这种转场效果，会水平摇晃界面0.25秒。\n    # blinds        垂直遮蔽原界面，耗时1秒。\n    # squares       以平面效果转场界面，耗时1秒。\n    # wipeleft      以指定方向擦除原界面。 (wiperight, wipeup, wipedown)\n    # slideleft     以指定方向滑入新场景。(slideright, slideup, slidedown)\n    # slideawayleft 以指定方向滑出旧场景。(slideawayright, slideawayup, slideawaydown)\n    # pushright     新场景把旧场景从指定的边界推出。(pushleft, pushup, pushdown)\n    # irisin        使用一个矩形iris显示新界面，或者隐藏旧界面。(irisout)\n# endregion\n'重新调用{i}scene{/i}则清除所有图像并显示一个背景图'\n'{i}with fade{/i}表示特效为淡出'\n# +-----------------------------------------------------------+\n# |topleft, reset               top                   topright|\n# |                                                           |\n# |                                                           |\n# |                                                           |\n# |                                                           |\n# |                          truecenter                       |\n# |                                                           |\n# |                                                           |\n# |                                                           |\n# |                                                           |\n# |left                   center, default                right|\n# +-----------------------------------------------------------+\n\n'背景音乐'\nplay music \"audio/Piano Sonata no. 11, K. 331 - I. Andante grazioso.mp3\" fadeout 1.0 fadein 1.0\n'fadeout and fadein分句用于旧音乐的淡出和新音乐的淡入'\n\nshow eileen at left\ne concerned \"在左侧显示人像\"\nshow eileen at right with move\ne concerned \"移动到右侧\"\n\ndefine l = Character(\"露西\", image=\"lucy\", color='#92b3a5')\nimage lucy happy = \"character/lucy happy.png\"\nimage lucy mad = \"character/lucy mad.png\"\n\nshow lucy happy at left\nl happy \"增加一个角色\"\n\nl mad \"对话合并    可以自定义样式\"(multiple=2)\ne happy \"对话合并\"(multiple=2)\n\n\"python语句\"\n\"单行用{i}${\\i}表示\"\n\"如{i}$ count = 0{\\i}\"\n$ count = 0\n'使用\\[{i}var{\\i}\\]调用变量'\n\"这里count = [count]\"\n\n\"Ren'Py中if-else可以直接使用\"\n\nif 0:\n    $ count = 0\nelif -1:\n    $ count = -1\nelse:\n    $ count = 1\n\n\"count现在是: [count]\"\n\n\"多行python语句\"\npython:\n    for i in range(1,5,2):\n        count += i\n        print(f'count现在是: {count}')\n\n\"count现在是: [count]\"\n\n\"使用多行python来读取输入\"\ndefine pov = Character(\"[povname]\")\npython:\n    # python 代码块\n    povname = renpy.input(\"你的名字是什么？\", length=32)\n    povname = povname.strip()\n\n    if not povname:\n        povname = \"122333\"\n\npov \"我的名字是 [povname]!\"\n\n\"使用manu来进行分支选择\"\n\"关于 import 引用，以下选项中描述错误的是( )\"\n$ ans = 0\nmenu:\n    'A. 使用 import numpy 引入 numpy 库':\n        $ ans = 1\n    'B. 可以使用 from numpy import Pi 引入 numpy 库':\n        $ ans = 2\n    'C. 使用 import numpy as np 引入 numpy 库，取别名为 np':\n        $ ans = 3\n    'D. import 保留字用于导入模块或者模块中的对象':\n        $ ans = 4\n\nif ans == 2:\n    \"回答正确\"\nelse:\n    \"回答错误\"\n\nnvl_narrator 'NVL模式'\nnvl_narrator 'NVL模式指的是一张背景图，全屏文本的模式'\nnvl_narrator '适合背景介绍，旁白等'\n\nnvl_narrator \"定义NVL内角色玛丽\"\ndefine m = Character(\"玛丽\", kind=nvl, color='#c8c8ff')\nm \"我是玛丽\"\n\ne 'NVL外的角色说话即离开NVL'\nm 'NVL内的角色说话回到NVL'\n\nnvl_narrator \"可以使用'nvl clear'来清空屏幕\"\nnvl clear\nnvl_narrator \"现在NVL的内容清空了\"\n\n\"下面介绍标签\"\n\"标签类似面向过程编程的标签，或者内存中的跳转\"\n\"面向过程编程中，按照自上而下的顺序逐行执行，也就是说，只有第一行命令执行之后，才可能执行第二行，第二行执行之后，才可以执行到第三行.....如此依次执行\"\n\"使用流程跳转\"\n\"jump语句用于将主控流程转入给定的脚本标签(label)处而call语句允许主控流程在执行完这次调用后，回到调用发生的脚本位置。\"\n\nmenu:\n    \"尝试jump跳转标签\":\n        jump .a1\n    \n    \"尝试call跳转标签\":\n        call .a1\n# 此处为游戏结尾。\n\"结尾\"\nreturn \nlabel .a1: “这里是a1标签”\nlabel .a2: “这里是a2标签” return\nlabel quit: e “退出标签quit” e “若该标签存在，当用户从主菜单退出游戏时该标签内容会被调用。” “{b}{size=45}{alpha=0.7}{color=#ed556a}END{/color}{/alpha}{/size}{/b}” return"
  },
  {
    "objectID": "posts/intro_to_python/intro_to_python.html",
    "href": "posts/intro_to_python/intro_to_python.html",
    "title": "Series: Intro to Python",
    "section": "",
    "text": "About this series\nCourse materials for [Intro to Python] in 2020 spring at Jiangxi University of Software Professional Technology The contents are showing below:\n\n\n\nSchedule\nTopic\nMaterial\nDescription\n\n\n\n\nWeek 00\nprerequisite\ninstallation\nFollow the instruction to create python virtual environment.\n\n\nWeek 01\nbasic information\njupyter notebook\nFollow the instruction to play with jupyter notebook.\n\n\nWeek 02\npython grammer\nPython grammer\nApply basic python grammer to calculate.\n\n\nWeek 03\npython grammer 2\nPython grammer 2\nClass in python, file IO in python.\n\n\nWeek 04\nnumpy\nNumpy\nIntroducing the Numpy package for calculation, also a little bit Data Structure.\n\n\nWeek 05\npandas\nPandas\nIntroducing the Numpy package.\n\n\nWeek 06\nMatplotlib 1\nMatplotlib 1\nIntroducing the Matplotlib package for ploting basic plots.\n\n\nWeek 07\nMatplotlib 2\nMatplotlib 2\nIntroducing the Matplotlib package for ploting, including 3D plot and animation.\n\n\nWeek 08\nweb crawler\nspiders\nIntroducing the requests and BeautifulSoup package for basic web crawler application.\n\n\nWeek 09\nregular expression\nregex\nBasic regular expressions using re package.\n\n\nWeek 10\nMidterm Exam\n—————————————-\n—————————————————————-\n\n\nWeek 11\nRen’py\nRen’py\nSimple sample Visual Novel Project with Ren’py instructions.\n\n\nWeek 12\nFalsk\nFalsk\nBasic web application using flask.\n\n\nWeek 13\nOpenCV\nOpenCV\nAdvaced vision applications using the OpenCV package."
  },
  {
    "objectID": "posts/misc/misc.html",
    "href": "posts/misc/misc.html",
    "title": "Miscellaneous",
    "section": "",
    "text": "This series contains miscellaneous materials I have.\n\n\n\n\n\n\n\n\n\n\n01 week01\n\n\n\n\n\nweek01 class materials, simple jupyter notebook tips\n\n\n\n\n\n\nOct 6, 2023\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blogs",
    "section": "",
    "text": "Series\n\nIntro to Python\n\n\nMachine Learning\n\n\nMisc\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nSep 9, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/misc/01.Private Server Setup notes.html",
    "href": "posts/misc/01.Private Server Setup notes.html",
    "title": "01 week01",
    "section": "",
    "text": "Device\n111 ### OS 111 ### OMV (Debian 11) 111 ### Docker 111 ### Nginx && NextCloud 111\nPrint your full name here to replace “Hello World!” 打印姓名\n\nprint(\"Hello World!\")\n\nHello World!\n\n\nview/hide line numbers in jupyter notebook 显示/隐藏行数\nclick View -&gt; click Toggle Line Numbers 点击View -&gt; 点击Toggle Line Numbers\n\n# This is line #5   这是第5行\n\nthe order of executing block changes results jupyter notebook 中未运行的代码块对后面代码块无影响\n\nimport numpy as np\n\n\nprint(np.__version__)\n\n1.26.0\n\n\nthe order of executing block changes results jupyter notebook 代码运行顺序对结果有影响\n\na=5\n\n\nprint(a)\n\n5\n\n\n\na=6\n\nIf you run the last code block before the print block, the output will show that a is 6."
  },
  {
    "objectID": "posts/intro_to_python/week00-install.html",
    "href": "posts/intro_to_python/week00-install.html",
    "title": "00 Installation",
    "section": "",
    "text": "1. Installation\ninstall.bat\n\n\nCode\n@echo off\nREM 声明采用UTF-8编码\nchcp 65001\nECHO.\nTITLE [环境安装]\ncolor 70\n@REM start/wait python-3.7.8-amd64.exe /passive PrependPath=1\n@REM pause\ncall python -m venv venv\ncall dir\npause\ncall venv\\Scripts\\activate\ncall python -m pip install --upgrade pip\ncall pip3 --trusted-host pypi.tuna.tsinghua.edu.cn install -i https://pypi.tuna.tsinghua.edu.cn/simple pandas\ncall pip3 --trusted-host pypi.tuna.tsinghua.edu.cn install -i https://pypi.tuna.tsinghua.edu.cn/simple jupyter\ncall pip3 list\ncall cmd\n\n@REM pause\n\n\nchcp 65001 : encoding in UTF-8 to present Chinese replace “python-3.7.8-amd64.exe” with your own python installation file /passive PrependPath=1 : sliencely extend the maximum length of path python -m venv venv : create a virtual environment\ninstall pandas and jupyter for now\n\n\n2. Execution\nI have provided 2 simple .bat file for activate the virtual environment and run jupyter notebook venv.bat\n\n\nCode\n@echo off\nREM 声明采用UTF-8编码\nchcp 65001\nECHO.\nTITLE [环境安装]\ncolor 70\n\ncall venv\\Scripts\\activate\necho \u001b[35mjupyter notebook stop\u001b[0m\ncall cmd\n\n\n@REM pause\n\n\nthe command ‘jupyter notebook stop’ is highlighted here this command could be used in this activated environment to stop the local sever of jupyter\n\n\nCode\n@echo off\nREM 声明采用UTF-8编码\nchcp 65001\nECHO.\nTITLE [运行]\ncolor 70\n\ncall venv\\Scripts\\activate\ncall jupyter notebook\n\n@REM pause\n\n\nSimplely double click this file to run jupyter notebook\n\n\n3. Possible Problems\nthere are two possible 1. jupyter notebook Bad file descriptor check the solution from this link (It’s in Chinese) 2. the default browser won’t prompt check the solution from this link (It’s in Chinese)"
  },
  {
    "objectID": "posts/intro_to_python/week12-flask.html",
    "href": "posts/intro_to_python/week12-flask.html",
    "title": "12 Flask",
    "section": "",
    "text": "Falsk\ninstall.bat\n#| code-fold: true #| eval: false # from tkinter import N from flask import Flask, render_template, request, redirect, url_for, make_response from urllib.request import urlopen\nimport re import numpy as np import pandas as pd import matplotlib.pyplot as plt\n\n\nhelp(Flask(name))\n\n\n| :param import_name: the name of the application package\n\n\n| :param static_url_path: can be used to specify a different path for the\n\n\n| static files on the web. Defaults to the name\n\n\n| of the static_folder folder.\n\n\n| :param static_folder: The folder with static files that is served at\n\n\n| static_url_path. Relative to the application root_path\n\n\n| or an absolute path. Defaults to 'static'.\n\n\n| :param static_host: the host to use when adding the static route.\n\n\n| Defaults to None. Required when using host_matching=True\n\n\n| with a static_folder configured.\n\n\n| :param host_matching: set url_map.host_matching attribute.\n\n\n| Defaults to False.\n\n\n| :param subdomain_matching: consider the subdomain relative to\n\n\n| :data:SERVER_NAME when matching routes. Defaults to False.\n\n\n| :param template_folder: the folder that contains the templates that should\n\n\n| be used by the application. Defaults to\n\n\n| 'templates' folder in the root path of the\n\n\n| application.\n\n\n| :param instance_path: An alternative instance path for the application.\n\n\n| By default the folder 'instance' next to the\n\n\n| package or module is assumed to be the instance\n\n\n| path.\n\n\n| :param instance_relative_config: if set to True relative filenames\n\n\n| for loading the config are assumed to\n\n\n| be relative to the instance path instead\n\n\n| of the application root.\n\n\n| :param root_path: The path to the root of the application files.\n\n\n| This should only be set manually when it can’t be detected\n\n\n| automatically, such as for namespace packages.\napp = Flask(name, template_folder=‘./’) print(app.template_folder)\n@app.route(‘/’) def hello_world(): return “\n\nHello World Flask!\n\n”\n@app.route(‘/h1’, methods=[‘POST’, ‘GET’]) def hello_h1(): return f”\n\nHello h1!\n\n”\n@app.route(‘/zqs’) def hello_zqs(): return “\n\nHello 张晴森!\n\n”\n@app.route(‘/’) def hello_age(age): return f”\n\nHello World Flask! age is {age}\n\n”\n@app.route(‘/hello/’) def hello_name(n): return f”\n\nHello {n}!\n\n”\n\n\n请求\n@app.route(‘/register’, methods=[‘GET’, ‘POST’]) def register(): if request.method == ‘POST’: name = request.form.get(‘name’) password = request.form.get(‘password’) print(f”name: {name}, password: {password}“) return redirect(url_for(”hello_zqs”)) # return f’Hello {name}!’ if request.method == ‘GET’: return render_template(‘Register.html’) # return ’’’&lt;!DOCTYPE html&gt; #\n\n#\n\n#\n\n#\n\n#\n\n#\n\n#\n\n#\n\nRegister\n\n#\n\n#\n\n#\n\n\n\n账号:\n\n\n\n\n\n\n\n\n密码:\n\n\n\n\n\n\n\n\n\n#\n\n#\n\n\n\n’’’\n\n\n重定向\n@app.route(‘/redirect/’) def redire_idx(a): if not a: return redirect(‘https://www.jxuspt.com/’) elif a % 2: return redirect(url_for(“hello_name”, n=f’{a} is odd’)) else: return redirect(url_for(“hello_name”, n=f’{a} is even’))\nurl = “http://jse.amstat.org/datasets/normtemp.dat.txt” html = urlopen(url).read().decode() pattern = re.compile(r’^(?P[]+?)+?(?P+?(?P)$‘, re.M) items = re.findall(pattern, html) df = pd.DataFrame(items, columns=[’体温’,’性别’,’心率’]) df[’体温’] = df[’体温’].apply(lambda x : round((float(x)-32)/1.8,2)) # 显示df @app.route(’/pd’) def from_pd(): # response = make_response(df.to_json(force_ascii=False)) # response.miteypte = ‘application/json’ # return response # return df.to_json(force_ascii=False) return df.to_html()\n@app.route(‘/mpl’) def mpl(): x_arr = np.arange(df.shape[0]) plt.scatter(x_arr, df[‘心率’], s=df[‘体温’], c=np.arctan2(df[‘体温’], x_arr), marker=‘8’) img_name = “scatter_plot.jpg” plt.savefig(‘./’+img_name) img_data = open(‘./’+img_name, “rb”).read() response = make_response(img_data) response.headers[‘Content-Type’] = ‘image/jpg’ return response\nif name == “main”: app.run(debug=True)"
  },
  {
    "objectID": "posts/machine_learning/01.Probability.html",
    "href": "posts/machine_learning/01.Probability.html",
    "title": "Predicting Diamond Prices: A Naive Bayes Approach",
    "section": "",
    "text": "Intro\nIn this project, we’re delving into the field of Probability and applying it to a practical problem: predicting diamond prices. Our primary tool for this task is the Naive Bayes algorithm, a powerful method in the realm of machine learning. The dataset we’re using is the diamonds dataset from Seaborn, which provides a rich variety of features for each diamond, including cut, color, and clarity. These features are categorical in nature, making them particularly well-suited for the Naive Bayes algorithm. Furthermore, the Naive Bayes algorithm is known for its computational efficiency, which allows for quick iterations and improvements in our predictive model. By combining these elements, we aim to build a model that can accurately predict diamond prices based on their features. This project not only showcases the practical application of Probability principles but also demonstrates the power and efficiency of the Naive Bayes algorithm in handling categorical data.\n\n\nNecessary Packages\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nplt.style.use('ggplot')\n# print(f\"List of seaborn datasets: \\n{sns.get_dataset_names()}\")\n\n\n\n\nData Process\n\nDownload Data\n\n\nCode\ndiamonds = sns.load_dataset('diamonds')\nprint(f\"There are {diamonds.isna().sum().sum()} missing values\")\ndiamonds\n\n\nThere are 0 missing values\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0\n0.23\nIdeal\nE\nSI2\n61.5\n55.0\n326\n3.95\n3.98\n2.43\n\n\n1\n0.21\nPremium\nE\nSI1\n59.8\n61.0\n326\n3.89\n3.84\n2.31\n\n\n2\n0.23\nGood\nE\nVS1\n56.9\n65.0\n327\n4.05\n4.07\n2.31\n\n\n3\n0.29\nPremium\nI\nVS2\n62.4\n58.0\n334\n4.20\n4.23\n2.63\n\n\n4\n0.31\nGood\nJ\nSI2\n63.3\n58.0\n335\n4.34\n4.35\n2.75\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n53935\n0.72\nIdeal\nD\nSI1\n60.8\n57.0\n2757\n5.75\n5.76\n3.50\n\n\n53936\n0.72\nGood\nD\nSI1\n63.1\n55.0\n2757\n5.69\n5.75\n3.61\n\n\n53937\n0.70\nVery Good\nD\nSI1\n62.8\n60.0\n2757\n5.66\n5.68\n3.56\n\n\n53938\n0.86\nPremium\nH\nSI2\n61.0\n58.0\n2757\n6.15\n6.12\n3.74\n\n\n53939\n0.75\nIdeal\nD\nSI2\n62.2\n55.0\n2757\n5.83\n5.87\n3.64\n\n\n\n\n53940 rows × 10 columns\n\n\n\nShow the categories and price\n\n\nCode\ngrouped = diamonds.groupby(['clarity', 'cut', 'color'])['price'].sum()\n\n# Sort the grouped data within each 'clarity' group\ngrouped_sorted = grouped.reset_index().sort_values(['clarity', 'price'], ascending=[True, False])\n\n# Unstack the sorted grouped data\ngrouped_sorted_unstacked = grouped_sorted.set_index(['clarity', 'cut', 'color']).unstack().fillna(0)\n\n# Create a stacked bar plot with sorted bars\ngrouped_sorted_unstacked.plot(kind='bar', stacked=True, figsize=(10, 6))\n\nplt.legend(title='Color', labels=['D', 'E', 'F', 'G', 'H', 'I', 'J'])\n# Add x-label, y-label, and title\nplt.xlabel('Clarity and Cut', fontname='serif', color='darkred',)\nplt.ylabel('Total Price', fontname='serif', color='darkred',)\nplt.title('Total Price by Clarity and Cut, Sorted within Each Clarity Group', fontname='serif', color='darkblue', fontsize=16)\nplt.show()\n\n\n\n\n\n\n\n\nz-score normalization\n\\(z-score = \\frac{x-\\mu}{\\sigma}\\)\n\n\nCode\ndf = diamonds.copy()\nscaler = StandardScaler()\ndf[['carat', 'depth', 'table', 'price', 'x', 'y', 'z']] = scaler.fit_transform(df[['carat', 'depth', 'table', 'price', 'x', 'y', 'z']])\ndf\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0\n-1.198168\nIdeal\nE\nSI2\n-0.174092\n-1.099672\n-0.904095\n-1.587837\n-1.536196\n-1.571129\n\n\n1\n-1.240361\nPremium\nE\nSI1\n-1.360738\n1.585529\n-0.904095\n-1.641325\n-1.658774\n-1.741175\n\n\n2\n-1.198168\nGood\nE\nVS1\n-3.385019\n3.375663\n-0.903844\n-1.498691\n-1.457395\n-1.741175\n\n\n3\n-1.071587\nPremium\nI\nVS2\n0.454133\n0.242928\n-0.902090\n-1.364971\n-1.317305\n-1.287720\n\n\n4\n-1.029394\nGood\nJ\nSI2\n1.082358\n0.242928\n-0.901839\n-1.240167\n-1.212238\n-1.117674\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n53935\n-0.164427\nIdeal\nD\nSI1\n-0.662711\n-0.204605\n-0.294731\n0.016798\n0.022304\n-0.054888\n\n\n53936\n-0.164427\nGood\nD\nSI1\n0.942753\n-1.099672\n-0.294731\n-0.036690\n0.013548\n0.100988\n\n\n53937\n-0.206621\nVery Good\nD\nSI1\n0.733344\n1.137995\n-0.294731\n-0.063434\n-0.047741\n0.030135\n\n\n53938\n0.130927\nPremium\nH\nSI2\n-0.523105\n0.242928\n-0.294731\n0.373383\n0.337506\n0.285204\n\n\n53939\n-0.101137\nIdeal\nD\nSI2\n0.314528\n-1.099672\n-0.294731\n0.088115\n0.118616\n0.143499\n\n\n\n\n53940 rows × 10 columns\n\n\n\n\n\nModel Defination\n\nPosterior Probability: \\[ P(C_k|X) = \\frac{P(X|C_k)P(C_k)}{P(X)}\\]\n\nPrior Probability of Class: \\[ P(C_k) = \\frac{\\text{Number of instances of class } C_k}{\\text{Total number of instances}} \\]\n\nLikelihood: \\[ P(x|C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{ -\\frac{(x-\\mu)^2}{2\\sigma^2} } \\]\n\nPrior Probability of Predictor: \\[ P(X) = \\sum_{k} P(X|C_k)P(C_k) \\]\n\n\n\nCode\ndef fit(X, y):\n  classes = np.unique(y)\n  parameters = []\n  for i, c in enumerate(classes):\n    X_c = X[y == c]\n    parameters.append({\n      'prior': X_c.shape[0] / X.shape[0],\n      'mean': X_c.mean(axis=0),\n      'var': X_c.var(axis=0)\n      })\n  return classes, parameters\n\ndef predict(X, classes, parameters):\n  N, D = X.shape\n  K = len(classes)\n  P = np.zeros((N, K))\n  for k in range(K):\n    P[:, k] = np.log(parameters[k]['prior'])\n    P[:, k] += -0.5 * np.sum(np.log(2. * np.pi * parameters[k]['var']))\n    P[:, k] += -0.5 * np.sum(((X - parameters[k]['mean']) ** 2) / (parameters[k]['var']), 1)\n  return np.argmax(P, 1)\n\n\n\nPrediction\nFirst, let’s do a test run on our model.\n\n\nCode\ncolumns = ['cut', 'clarity', 'color']\ncolumne_name = columns[0]\nX = df[['carat', 'depth', 'table', 'price', 'x', 'y', 'z']]\ny = df[columne_name]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nclasses, parameters = fit(X_train, y_train)\npredictions = predict(X_test, classes, parameters)\npredictions_label = classes[predictions]\naccuracy = accuracy_score(y_test, predictions_label)\nprint(f\"Accuracy on {columne_name} is {accuracy:.2f}\")\n\n\nAccuracy on cut is 0.57\n\n\nNext, we can perform prediction on each category of [‘cut’, ‘clarity’, ‘color’].\n\n\nCode\ndf = diamonds.copy()\nscaler = StandardScaler()\ndf[['carat', 'depth', 'table', 'x', 'y', 'z']] = scaler.fit_transform(df[['carat', 'depth', 'table', 'x', 'y', 'z']])\n\nX = df[['carat', 'depth', 'table', 'x', 'y', 'z']]\ny = df[['cut', 'clarity', 'color']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nclasses_cut, parameters_cut = fit(X_train, y_train['cut'])\npredictions_cut = predict(X_test, classes_cut, parameters_cut)\n\nclasses_clarity, parameters_clarity = fit(X_train, y_train['clarity'])\npredictions_clarity = predict(X_test, classes_clarity, parameters_clarity)\n\nclasses_color, parameters_color = fit(X_train, y_train['color'])\npredictions_color = predict(X_test, classes_color, parameters_color)\n\n\nLet’s try plot this:\n\n\nCode\ncount_df = pd.DataFrame()\ncount_df['correct_cut'] = (y_test['cut'] == classes_cut[predictions_cut]).map({True: 'Correct', False: 'Incorrect'})\ncount_df['correct_clarity'] = (y_test['clarity'] == classes_clarity[predictions_clarity]).map({True: 'Correct', False: 'Incorrect'})\ncount_df['correct_color'] = (y_test['color'] == classes_color[predictions_color]).map({True: 'Correct', False: 'Incorrect'})\n\n# Count the number of correct predictions for each feature\ncorrect_cut = count_df['correct_cut'].value_counts()['Correct']\ncorrect_clarity = count_df['correct_clarity'].value_counts()['Correct']\ncorrect_color = count_df['correct_color'].value_counts()['Correct']\n\n# Count the number of incorrect predictions for each feature\nincorrect_cut = count_df['correct_cut'].value_counts()['Incorrect']\nincorrect_clarity = count_df['correct_clarity'].value_counts()['Incorrect']\nincorrect_color = count_df['correct_color'].value_counts()['Incorrect']\n\naccuracy_cut = accuracy_score(y_test['cut'], classes_cut[predictions_cut])\naccuracy_clarity = accuracy_score(y_test['clarity'], classes_clarity[predictions_clarity])\naccuracy_color = accuracy_score(y_test['color'], classes_color[predictions_color])\n\n# Create a DataFrame for the counts\ndata = {'Correct': [correct_cut, correct_clarity, correct_color],\n        'Incorrect': [incorrect_cut, incorrect_clarity, incorrect_color]}\ndf_counts = pd.DataFrame(data, index=['cut', 'clarity', 'color'])\n\ncolors=plt.get_cmap('Paired', 2)\n\ndf_counts.plot(kind='barh', stacked=True, color=colors.colors)\n\naccuracies = [accuracy_cut, accuracy_clarity, accuracy_color]\nfor i, (v, accuracy) in enumerate(zip(df_counts['Correct'], accuracies)):\n  plt.text(v, i, f' {v} ({accuracy*100:.2f}%)', va='center')\n\nplt.xlabel('Count', fontname='serif', color='darkred',)\nplt.ylabel('Category', fontname='serif', color='darkred',)\nplt.title('Prediction Correctness', fontname='serif', color='darkblue', fontsize=16)\nplt.show()\n\n\n\n\n\nI think a Venn diagram is better in this case to show their overlap of correctness.\n\n\nCode\nfrom matplotlib_venn import venn3\nimport matplotlib.patches as mpatches\n\n# Count the number of correct predictions for each pair of features\ncorrect_cut_clarity = count_df[(count_df['correct_cut'] == 'Correct') & (count_df['correct_clarity'] == 'Correct')].shape[0]\ncorrect_cut_color = count_df[(count_df['correct_cut'] == 'Correct') & (count_df['correct_color'] == 'Correct')].shape[0]\ncorrect_clarity_color = count_df[(count_df['correct_clarity'] == 'Correct') & (count_df['correct_color'] == 'Correct')].shape[0]\n\n# Count the number of correct predictions for all three features\ncorrect_all = count_df[(count_df['correct_cut'] == 'Correct') & (count_df['correct_clarity'] == 'Correct') & (count_df['correct_color'] == 'Correct')].shape[0]\n\n# Create the Venn diagram\nplt.figure(figsize=(8,8))\nvenn = venn3(subsets=(correct_cut, correct_clarity, correct_color, correct_cut_clarity, correct_cut_color, correct_clarity_color, correct_all), set_labels=('Cut', 'Clarity', 'Color'))\n\nfor text in venn.set_labels:\n  text.set_fontname('serif')\n  text.set_color('darkred')\n\nplt.title('Correct Predictions', fontname='serif', color='darkblue', fontsize=16)\n\n# Create a custom legend\nlegend_elements = [mpatches.Patch(color=venn.get_patch_by_id('100').get_facecolor(), label='Cut'),\nmpatches.Patch(color=venn.get_patch_by_id('010').get_facecolor(), label='Clarity'),\nmpatches.Patch(color=venn.get_patch_by_id('001').get_facecolor(), label='Color')]\n\n# Calculate the count of the union of correct predictions\ncorrect_union = count_df[(count_df['correct_cut'] == 'Correct') | (count_df['correct_clarity'] == 'Correct') | (count_df['correct_color'] == 'Correct')].shape[0]\n\n# Calculate the count of the complementary set\ncomplement_count = len(y_test) - correct_union\n\n# Add the count of the complementary set to the plot\nplt.text(0.2, 0.5, f'Complementary set \\nin Universe\\n({complement_count})', horizontalalignment='center', verticalalignment='center')\n\nplt.legend(handles=legend_elements, loc='best')\n\nplt.show()\n\n\n\n\n\nend.\ngo back"
  },
  {
    "objectID": "posts/machine_learning/04.Classification.html",
    "href": "posts/machine_learning/04.Classification.html",
    "title": "01 week01",
    "section": "",
    "text": "Intro\nIn this project, we aim to apply the One-Class Support Vector Machine (SVM) algorithm for anomaly detection on the Dow Jones dataset from seaborn. The dataset comprises two columns: Date and Price, making it a suitable candidate for time-series anomaly detection. The One-Class SVM method is particularly effective for such tasks as it defines a boundary around normal data, and any data point falling outside this boundary is considered an anomaly. By leveraging this method, we hope to identify unusual patterns in the Dow Jones index that could potentially indicate significant financial events or market abnormalities. This approach could provide valuable insights for financial analysis and decision-making processes. Stay tuned as we dive into the details of implementing and optimizing the One-Class SVM model for our dataset.\n\n\nNecessary Packages\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nplt.style.use('ggplot')\nprint(f\"List of seaborn datasets: \\n{sns.get_dataset_names()}\")\n\n\nList of seaborn datasets: \n['anagrams', 'anscombe', 'attention', 'brain_networks', 'car_crashes', 'diamonds', 'dots', 'dowjones', 'exercise', 'flights', 'fmri', 'geyser', 'glue', 'healthexp', 'iris', 'mpg', 'penguins', 'planets', 'seaice', 'taxis', 'tips', 'titanic']\n\n\nPrint your full name here to replace “Hello World!” 打印姓名\n\n\nCode\nprint(\"Hello World!\")\n\n\nHello World!\n\n\nview/hide line numbers in jupyter notebook 显示/隐藏行数\nclick View -&gt; click Toggle Line Numbers 点击View -&gt; 点击Toggle Line Numbers\n\n\nCode\n# This is line #5   这是第5行\n\n\nthe order of executing block changes results jupyter notebook 中未运行的代码块对后面代码块无影响\n\n\nCode\nimport numpy as np\n\n\n\n\nCode\nprint(np.__version__)\n\n\n1.26.0\n\n\nthe order of executing block changes results jupyter notebook 代码运行顺序对结果有影响\n\n\nCode\na=5\n\n\n\n\nCode\nprint(a)\n\n\n5\n\n\n\n\nCode\na=6\n\n\nIf you run the last code block before the print block, the output will show that a is 6."
  },
  {
    "objectID": "posts/machine_learning/05.Anomaly_outlier detection.html",
    "href": "posts/machine_learning/05.Anomaly_outlier detection.html",
    "title": "Outlier Detection in Dow Jones using Support Vector Machines",
    "section": "",
    "text": "Intro\nIn this project, we aim to apply the One-Class Support Vector Machine (SVM) algorithm for anomaly detection on the Dow Jones dataset from seaborn. The dataset comprises two columns: Date and Price, making it a suitable candidate for time-series anomaly detection. The One-Class SVM method is particularly effective for such tasks as it defines a boundary around normal data, and any data point falling outside this boundary is considered an anomaly. By leveraging this method, we hope to identify unusual patterns in the Dow Jones index that could potentially indicate significant financial events or market abnormalities. This approach could provide valuable insights for financial analysis and decision-making processes. Stay tuned as we dive into the details of implementing and optimizing the One-Class SVM model for our dataset.\n\n\nNecessary Packages\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nplt.style.use('ggplot')\n# print(f\"List of seaborn datasets: \\n{sns.get_dataset_names()}\")\n\n\n\n\nData Process\n\nDownload Data\n\n\nCode\ndowjones_raw = sns.load_dataset('dowjones')\nprint(f\"There are {dowjones_raw.isna().sum().sum()} missing values\")\ndowjones_raw\n\n\nThere are 0 missing values\n\n\n\n\n\n\n\n\n\nDate\nPrice\n\n\n\n\n0\n1914-12-01\n55.00\n\n\n1\n1915-01-01\n56.55\n\n\n2\n1915-02-01\n56.00\n\n\n3\n1915-03-01\n58.30\n\n\n4\n1915-04-01\n66.45\n\n\n...\n...\n...\n\n\n644\n1968-08-01\n883.72\n\n\n645\n1968-09-01\n922.80\n\n\n646\n1968-10-01\n955.47\n\n\n647\n1968-11-01\n964.12\n\n\n648\n1968-12-01\n965.39\n\n\n\n\n649 rows × 2 columns\n\n\n\nNext, we want to do some pre-process, although the dowjones dataset only contain 2 columns, we want to: - set the Date column to be the index\n\n\nCode\ndowjones = dowjones_raw.copy()\ndowjones.set_index('Date', drop=True, inplace=True)\n\nplt.figure()\nplt.plot(dowjones, label=\"dowjones\", c='g')\nplt.xlabel('Date',  fontname='serif', color='darkred')\nplt.ylabel('Price',  fontname='serif', color='darkred')\nplt.title('Dowjones Price', fontname='serif', color='darkblue', fontsize=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFrom the plot, we can see the price data shows growth from 55 to over 800, with a peak around 1930. Direct anomaly detection on the price data would only highlight the mid-range prices, so we’re performing feature engineering on the dataset to normalize the prices.\n\nfeature engineering on Price\n\n\n\nCode\ndef minmax_scaler(window, value):\n  return (value - window.min()) / (window.max() - window.min())\n\ndef zscore_scaler(window, value):\n  z_score = (value - np.mean(window)) / (np.std(window))\n  return z_score\n\n\ndef add_normalized_price(price, period):\n    scaler = MinMaxScaler()\n    price_normalized = pd.Series(index=price.index)\n    half_period = np.round(period/2).astype(np.int16)\n\n    for i, e in enumerate(price):\n        start = max(0, i-half_period)\n        end = min(price.shape[0], i+half_period)\n\n        window = price.iloc[start:end]\n        nromed_value = minmax_scaler(window, price.iloc[i])\n        price_normalized.iloc[i] = nromed_value\n\n    return price_normalized\n\n\n    \n\nprice_norm = add_normalized_price(dowjones['Price'], 12)\ndowjones['Normalized_Price'] = price_norm\ndowjones\n\n\n\n\n\n\n\n\n\nPrice\nNormalized_Price\n\n\nDate\n\n\n\n\n\n\n1914-12-01\n55.00\n0.000000\n\n\n1915-01-01\n56.55\n0.115672\n\n\n1915-02-01\n56.00\n0.059347\n\n\n1915-03-01\n58.30\n0.136082\n\n\n1915-04-01\n66.45\n0.375410\n\n\n...\n...\n...\n\n\n1968-08-01\n883.72\n0.374799\n\n\n1968-09-01\n922.80\n0.673965\n\n\n1968-10-01\n955.47\n0.878536\n\n\n1968-11-01\n964.12\n0.984450\n\n\n1968-12-01\n965.39\n1.000000\n\n\n\n\n649 rows × 2 columns\n\n\n\nEach price is normalized within a 12-month time frame, with the price ideally situated in the middle of this period. This approach aids in anomaly detection by identifying prices that are extreme for their specific time context, potentially signaling significant market events.\n\n\nData Preview\n\n\nCode\ncolors=plt.get_cmap('Paired', 2)\nfig, ax1 = plt.subplots()\n\nax1.plot(dowjones.index, dowjones.Price, color=colors(0), alpha=0.9, lw=3)\nax1.set_ylabel('Price', color=colors(0))\nax1.tick_params(axis='y', labelcolor=colors(0))\nax1.set_ylim(-200, 1000)\n\nax2 = ax1.twinx()\ndeviation = np.abs(dowjones.Normalized_Price-0.5)\nax2.fill_between(dowjones.index, 0, deviation, color=colors(1))\nax2.set_ylabel('Deviation', color=colors(1))\nax2.tick_params(axis='y', labelcolor=colors(1))\nax2.set_ylim(-3, 0.6)\nax2.grid()\nax2.invert_yaxis()\n\nplt.title(\"Dow Jones Index: Price and Normalized Deviation Over Time\", fontname='serif', color='darkblue', fontsize=16)\nplt.show()\n\n\n\n\n\nTry use IQR to detect outlier in dataset\n\n\nCode\n# column_name = 'Price'\ncolumn_name = 'Normalized_Price'\nq1 = dowjones[column_name].quantile(0.25)\nq3 = dowjones[column_name].quantile(0.75)\niqr = q3 - q1\n\nprint(f\"Q1 and Q3 of {column_name} column is {q1:.2f} & {q3:.2f}.\\nIQR is {iqr:.2f}\")\n\nlower_bound = q1 - 1.5 * iqr\nupper_bound = q3 + 1.5 * iqr\nprint(f\"Any value &lt; {lower_bound:.2f} and  &gt; {upper_bound:.2f} is an outlier.\")\n\noutliers = dowjones[(dowjones[column_name] &lt; lower_bound) | (dowjones[column_name] &gt; upper_bound)]\nprint(f\"There are {outliers.shape[0]} outliers.\")\n\n\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=dowjones[column_name])\n\nplt.axvline(x=lower_bound, color='r', linestyle='--')\nplt.axvline(x=upper_bound, color='r', linestyle='--')\n\nticks = list(plt.xticks()[0]) + [lower_bound] + [upper_bound]\nlabels = [str(t) for t in ticks]\nlabels[-2] = f'\\nQ1: {lower_bound:.2f}'\nlabels[-1] = f'\\nQ2: {upper_bound:.2f}'\nplt.xticks(ticks, labels)\n\nplt.xlabel(column_name, fontname='serif', color='darkred')\nplt.ylabel('Frequency', fontname='serif', color='darkred')\nplt.title(f'Box plot of {column_name} with outliers', fontname='serif', color='darkblue', fontsize=16)\n\n# Show the plot\nplt.show()\n\n\nQ1 and Q3 of Normalized_Price column is 0.32 & 0.70.\nIQR is 0.38\nAny value &lt; -0.24 and  &gt; 1.26 is an outlier.\nThere are 0 outliers.\n\n\n\n\n\nThis plot shows the price and the deviation, notice that deviation is the absolute value of (normalized price - 0.5) this may shows how far the price is from the center of the year period.\n\n\n\nSupport Vector Machine (SVM)\n\nLoss Function: \\(\\text{Loss} = \\frac{1}{2} \\sum_{i=1}^{n} w_i^2 - \\rho\\)\nUpdate Weights: \\(w = w + \\frac{\\sum_{i=1}^{n} X_i}{n} - w\\)\nCompute Rho: \\(\\rho = \\text{percentile}(\\text{distance}, \\nu \\times 100)\\)\nPredict Function: \\[\n\\text{{prediction}} =\n\\begin{cases}\n1 & \\text{{if }} x \\cdot w \\geq \\rho \\\\\n-1 & \\text{{otherwise}}\n\\end{cases}\n\\]\n\n\n\nCode\ndef compute_loss(X, w, rho):\n    return (1/2) * np.sum(w**2) - rho\n\ndef compute_rho(X, w, nu):\n    dist = np.dot(X, w)\n    return np.percentile(dist, nu * 100)\n\ndef update_weights(X, w, rho):\n    dist = np.dot(X, w)\n    outliers = dist &lt; rho\n    if np.any(outliers):\n      w += np.sum(X[outliers], axis=0)/len(X[outliers]) - w\n    else:\n      w += np.random.normal(0, 1e-18, size=w.shape)\n\n    return w\n\ndef fit(X, nu, max_iter, tol):\n    n_samples, n_features = X.shape\n    w = np.zeros(n_features)\n    rho = np.min(X)\n\n    for _ in range(max_iter):\n        loss_old = compute_loss(X, w, rho)\n        w = update_weights(X, w, rho)\n        rho = compute_rho(X, w, nu)\n        loss_new = compute_loss(X, w, rho)\n\n        if np.abs(loss_new - loss_old) &lt; tol:\n            break\n\n    return w, rho\n\ndef predict(X, w, rho):\n    dist = np.dot(X, w)\n    return np.where(dist &gt;= rho, 1, 0)\n\n\nfit the model\n\n\nCode\ncolumn_name = 'Price'\nX = dowjones[column_name].values.reshape(-1, 1)\n\nnu = 0.05\nmax_iter = 1000\ntol = 1e-3\n\nw, rho = fit(X, nu, max_iter, tol)\n\npredictions = predict(X, w, rho)\nprint(f\"there is {X.shape[0] - predictions.sum()} outliers detected by column {column_name}\")\n\noutlier_indices = np.where(predictions == 0)[0]\noutlier = dowjones.iloc[outlier_indices]\n# outlier\n\n\nthere is 33 outliers detected by column Price\n\n\n\n\nCode\nplt.figure()\nplt.plot(dowjones.Price, label=\"Dow Jones\", c='g')\nplt.scatter(outlier.index, outlier.Price, color='r', label='Outliers', s=10, marker='8')\nplt.xlabel('Date',  fontname='serif', color='darkred')\nplt.ylabel('Price',  fontname='serif', color='darkred')\nplt.title(f'Dow Jones Price with Anomalies by {column_name}', fontname='serif', color='darkblue', fontsize=16)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nCode\ncolumn_name = 'Normalized_Price'\nX = dowjones[column_name].values.reshape(-1, 1)\n\nnu = 0.05\nmax_iter = 1000\ntol = 1e-3\n\nw, rho = fit(X, nu, max_iter, tol)\n\npredictions = predict(X, w, rho)\nprint(f\"there is {X.shape[0] - predictions.sum()} outliers detected by column {column_name}\")\n\noutlier_indices = np.where(predictions == 0)[0]\noutlier = dowjones.iloc[outlier_indices]\n# outlier\n\n\nthere is 33 outliers detected by column Normalized_Price\n\n\n\n\nCode\nplt.figure()\nplt.plot(dowjones.Price, label=\"Dow Jones\", c='g')\nplt.scatter(outlier.index, outlier.Price, color='r', label='Outliers', s=10, marker='8')\nplt.xlabel('Date',  fontname='serif', color='darkred')\nplt.ylabel('Price',  fontname='serif', color='darkred')\nplt.title(f'Dow Jones Price with Anomalies by {column_name}', fontname='serif', color='darkblue', fontsize=16)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nend.\ngo back"
  },
  {
    "objectID": "posts/machine_learning/03.Linear and nonlinear regression 2.html",
    "href": "posts/machine_learning/03.Linear and nonlinear regression 2.html",
    "title": "Harnessing Momentum: Predictive Analysis of Stock Prices Using Linear and Nonlinear Regression II",
    "section": "",
    "text": "Continue from previous blog\n\nNecessary Packages\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.metrics import accuracy_score\n\n\n\n\nData Process\n\nDownload Data\nHere we download the price data of the stock\n\n\nCode\n# stock = 'AAPL'\nstock = 'GOOGL'\nperiod = '1d'\nstart = '2000-1-1'\nend = '2022-12-31'\n\ntickerData = yf.Ticker(stock)\n\ntickerDf = tickerData.history(period=period, start=start, end=end)\n\ntickerDf.index = pd.to_datetime(tickerDf.index)\n\nprint(f\"There are {tickerDf.isna().sum().sum()} missing values\")\ntickerDf\n\n\nThere are 0 missing values\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\nDividends\nStock Splits\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n2004-08-19 00:00:00-04:00\n2.502503\n2.604104\n2.401401\n2.511011\n893181924\n0.0\n0.0\n\n\n2004-08-20 00:00:00-04:00\n2.527778\n2.729730\n2.515015\n2.710460\n456686856\n0.0\n0.0\n\n\n2004-08-23 00:00:00-04:00\n2.771522\n2.839840\n2.728979\n2.737738\n365122512\n0.0\n0.0\n\n\n2004-08-24 00:00:00-04:00\n2.783784\n2.792793\n2.591842\n2.624374\n304946748\n0.0\n0.0\n\n\n2004-08-25 00:00:00-04:00\n2.626627\n2.702703\n2.599600\n2.652653\n183772044\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2022-12-23 00:00:00-05:00\n87.110001\n89.550003\n87.070000\n89.230003\n23003000\n0.0\n0.0\n\n\n2022-12-27 00:00:00-05:00\n88.800003\n88.940002\n87.010002\n87.389999\n20097300\n0.0\n0.0\n\n\n2022-12-28 00:00:00-05:00\n86.980003\n88.040001\n85.940002\n86.019997\n19523200\n0.0\n0.0\n\n\n2022-12-29 00:00:00-05:00\n86.620003\n88.849998\n86.610001\n88.449997\n23333500\n0.0\n0.0\n\n\n2022-12-30 00:00:00-05:00\n86.980003\n88.300003\n86.570000\n88.230003\n23986300\n0.0\n0.0\n\n\n\n\n4625 rows × 7 columns\n\n\n\n\n\nPresent Data\n\n\nCode\nclose_arr = tickerDf['Close']\n\nfig = plt.figure()\nplt.plot(close_arr.index, close_arr, label=stock)\nplt.xlabel(\"Date\", fontname='serif', color='darkred')\nplt.xticks(rotation=30)\nplt.ylabel(\"Price($)\", fontname='serif', color='darkred')\nplt.title(f\"{stock} price VS. Date\", fontname='serif', color='darkblue', fontsize=16)\nplt.legend()\nplt.plot()\n\n\n[]\n\n\n\n\n\n\n\nPresent Cumulative Return\n\\(return = \\frac{Current\\;price\\;-\\;Original\\;price}{Original\\;price}\\)\n\n\nCode\nreturn_arr = tickerDf['Close'].pct_change().dropna()\nreturn_arr = (1+return_arr).cumprod()\n\nfig = plt.figure()\nplt.plot(return_arr.index, return_arr, label=stock)\nplt.xlabel(\"Date\", fontname='serif', color='darkred')\nplt.xticks(rotation=30)\nplt.ylabel(\"Cumulative\\nReturn\", fontname='serif', color='darkred')\nplt.title(f\"{stock} return VS. Date\", fontname='serif', color='darkblue', fontsize=16)\nplt.legend()\nplt.plot()\n\n\n[]\n\n\n\n\n\nTherefore, if we trade the stock everyday, and ignore the trading fee, this is how the principal (initial amount of investment) growth during this peroid.\n\n\nRetrieve Factors\nLook at the product of returns which to capture momentum effects.\nNotice that there are about 252 trading days in one year.\n\nday_momentum: This is the one-day lagged return. It measures the momentum from the previous day.\nweek_momentum: This is the product of the past 5 days’ returns, shifted forward by one day. It measures the momentum from the past week.\nmonth_momentum: This is the product of the past 21 days’ returns, shifted forward by one day. It measures the momentum from the past month.\nquarter_momentum: This is the product of the past 63 days’ returns, shifted forward by one day. It measures the momentum from the past quarter.\nyear_momentum: This is the product of the past 252 days’ returns, shifted forward by one day. It measures the momentum from the past year.\n52_weeks_high_low_ratio: This is the ratio of the difference between the current close price and the 52-week low to the range of the 52-week high and low. It measures how close the current price is to the 52-week high relative to the 52-week range.\n\n52-week high:The 52-week high momentum strategy is a trading strategy where investors go long or short on a stock when its current price is near (or far from) its 52-week high. This can be measured using the formula:\n\\(\\frac{P_{(i, t-1)}}{High_{(i,t-1)}}\\)\nwhere\\(P_{(i, t-1)}\\) is is the price of the stock at time t−1, and \\(High_{(i,t-1)}\\) is the highest price the stock reached in the past 52 weeks.\n\n\nCode\ndf = pd.DataFrame()\ndf['return'] = tickerDf['Close'].pct_change()\n\n# make sure not using today's data to predict today's result\ndf['day_momentum'] = (1 + df['return']).shift(1) - 1\ndf['week_momentum'] = (1 + df['return']).rolling(window=5).apply(np.prod, raw=True).shift(1) - 1\ndf['month_momentum'] = (1 + df['return']).rolling(window=21).apply(np.prod, raw=True).shift(1) - 1\ndf['quarter_momentum'] = (1 + df['return']).rolling(window=63).apply(np.prod, raw=True).shift(1) - 1\ndf['year_momentum'] = (1 + df['return']).rolling(window=252).apply(np.prod, raw=True).shift(1) - 1\ndf['52_weeks_high_low_ratio'] = (tickerDf['Close'] - tickerDf['Low'].rolling(window=252).min()) / (tickerDf['High'].rolling(window=252).max() - tickerDf['Low'].rolling(window=252).min())\ndf\n\n\n\n\n\n\n\n\n\nreturn\nday_momentum\nweek_momentum\nmonth_momentum\nquarter_momentum\nyear_momentum\n52_weeks_high_low_ratio\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n2004-08-19 00:00:00-04:00\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2004-08-20 00:00:00-04:00\n0.079430\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2004-08-23 00:00:00-04:00\n0.010064\n0.079430\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2004-08-24 00:00:00-04:00\n-0.041408\n0.010064\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2004-08-25 00:00:00-04:00\n0.010776\n-0.041408\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2022-12-23 00:00:00-05:00\n0.016750\n-0.020317\n-0.034118\n-0.095724\n-0.111201\n-0.400608\n0.086356\n\n\n2022-12-27 00:00:00-05:00\n-0.020621\n0.016750\n-0.011411\n-0.093744\n-0.091066\n-0.392648\n0.059379\n\n\n2022-12-28 00:00:00-05:00\n-0.015677\n-0.020621\n-0.011872\n-0.103324\n-0.103692\n-0.409154\n0.039292\n\n\n2022-12-29 00:00:00-05:00\n0.028249\n-0.015677\n-0.033700\n-0.104425\n-0.140230\n-0.413581\n0.074920\n\n\n2022-12-30 00:00:00-05:00\n-0.002487\n0.028249\n-0.012614\n-0.070806\n-0.092076\n-0.396884\n0.071694\n\n\n\n\n4625 rows × 7 columns\n\n\n\n\n\nImprove Factors\n\nSmoothing: Smoothing the momentum calculations with a moving average to reduce noise.\nRisk adjustment: Adjust the momentum calculations for risk. Divide the momentum by the standard deviation (volatility) of returns over the past year to get a risk-adjusted momentum measure.\n\n\n\nCode\n# Smoothing with a moving average\ndf['day_momentum_smooth'] = df['day_momentum'].rolling(window=5).mean()\ndf['week_momentum_smooth'] = df['week_momentum'].rolling(window=5).mean()\ndf['month_momentum_smooth'] = df['month_momentum'].rolling(window=5).mean()\ndf['quarter_momentum_smooth'] = df['quarter_momentum'].rolling(window=5).mean()\ndf['year_momentum_smooth'] = df['year_momentum'].rolling(window=5).mean()\n\n# Risk adjustment\ndf['day_momentum_risk_adj'] = df['day_momentum'] / df['return'].rolling(window=252).std()\ndf['week_momentum_risk_adj'] = df['week_momentum'] / df['return'].rolling(window=252).std()\ndf['month_momentum_risk_adj'] = df['month_momentum'] / df['return'].rolling(window=252).std()\ndf['quarter_momentum_risk_adj'] = df['quarter_momentum'] / df['return'].rolling(window=252).std()\ndf['year_momentum_risk_adj'] = df['year_momentum'] / df['return'].rolling(window=252).std()\n\n\n\n\nNormalize or Standardize the factors\n\n\nCode\nscaler = StandardScaler()\n# scaler = MinMaxScaler()\ndf_scaled = df.dropna()\nreturn_arr = df_scaled['return']\ndf_scaled = df_scaled[df_scaled.columns[1:]]\ndf_scaled = pd.DataFrame(scaler.fit_transform(df_scaled), columns=df.columns[1:])\ndf_scaled.set_index(return_arr.index, inplace=True)\ndf_scaled\n\n\n\n\n\n\n\n\n\nday_momentum\nweek_momentum\nmonth_momentum\nquarter_momentum\nyear_momentum\n52_weeks_high_low_ratio\nday_momentum_smooth\nweek_momentum_smooth\nmonth_momentum_smooth\nquarter_momentum_smooth\nyear_momentum_smooth\nday_momentum_risk_adj\nweek_momentum_risk_adj\nmonth_momentum_risk_adj\nquarter_momentum_risk_adj\nyear_momentum_risk_adj\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2005-08-25 00:00:00-04:00\n0.526098\n-0.310159\n-0.747161\n0.255181\n4.249616\n0.582602\n-0.297116\n-0.796104\n-1.230024\n0.490876\n4.181518\n0.363544\n-0.248104\n-0.599921\n0.038821\n2.899085\n\n\n2005-08-26 00:00:00-04:00\n-0.036394\n0.137104\n-0.779210\n0.303679\n4.109359\n0.599627\n0.146940\n-0.653593\n-1.091965\n0.374525\n4.078954\n-0.039404\n0.063938\n-0.623066\n0.073999\n2.797304\n\n\n2005-08-29 00:00:00-04:00\n0.145344\n0.223297\n-0.602729\n0.131622\n4.266773\n0.683374\n0.232511\n-0.373545\n-0.903033\n0.240201\n4.130536\n0.091657\n0.125271\n-0.499924\n-0.047094\n2.937002\n\n\n2005-08-30 00:00:00-04:00\n0.869116\n1.206301\n-0.162035\n-0.051973\n4.732813\n0.663083\n1.184219\n0.157565\n-0.658245\n0.193399\n4.323529\n0.613407\n0.814937\n-0.186947\n-0.178942\n3.292229\n\n\n2005-08-31 00:00:00-04:00\n-0.256740\n0.585827\n-0.372427\n-0.357315\n4.668738\n0.641242\n0.581636\n0.452240\n-0.556265\n0.056409\n4.407314\n-0.198509\n0.380404\n-0.336556\n-0.398118\n3.250344\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2022-12-23 00:00:00-05:00\n-1.115868\n-0.931891\n-1.356665\n-1.131538\n-1.917170\n-2.250567\n-0.938167\n-1.568035\n-1.299998\n-1.170135\n-1.859593\n-0.855955\n-0.714464\n-1.076112\n-0.981907\n-1.833808\n\n\n2022-12-27 00:00:00-05:00\n0.846724\n-0.372649\n-1.332556\n-0.988083\n-1.893415\n-2.352100\n-0.361857\n-1.469678\n-1.317399\n-1.122054\n-1.870063\n0.628237\n-0.302470\n-1.057324\n-0.873380\n-1.813853\n\n\n2022-12-28 00:00:00-05:00\n-1.131957\n-0.384004\n-1.449202\n-1.078040\n-1.942674\n-2.427698\n-0.373133\n-1.225992\n-1.323680\n-1.090390\n-1.889887\n-0.866906\n-0.310713\n-1.143680\n-0.940787\n-1.852810\n\n\n2022-12-29 00:00:00-05:00\n-0.870187\n-0.921594\n-1.462599\n-1.338362\n-1.955887\n-2.293609\n-0.921832\n-1.011914\n-1.369715\n-1.143779\n-1.915115\n-0.667248\n-0.704125\n-1.150777\n-1.134300\n-1.860390\n\n\n2022-12-30 00:00:00-05:00\n1.455559\n-0.402278\n-1.053288\n-0.995273\n-1.906056\n-2.305748\n-0.379955\n-0.739137\n-1.389635\n-1.124418\n-1.927815\n1.084415\n-0.323457\n-0.847154\n-0.877042\n-1.820619\n\n\n\n\n4368 rows × 16 columns\n\n\n\nLet’s see if the factors are correlated to our target\n\n\nCode\nfor e in df_scaled.columns:\n  factor = df_scaled[e]\n  corr_coef = np.corrcoef(factor, return_arr)[0, 1]\n  if np.abs(corr_coef) &gt; 0.1:\n    print(f\"The correlation coefficient between {e} and target is {corr_coef:.2f}\")\n\n\nThe correlation coefficient between 52_weeks_high_low_ratio and target is 0.13\n\n\nPredict the return as target y for linear regression, and predict net return (0 for positive, 1 for negative) as target for logistic regression.\nSplit train-test datasets to evaluate the performance of each model.\n\n\nCode\nX = df_scaled\ny_linear = return_arr\ny_logistic = normalize_and_binarize(return_arr)\n\nX_train_linear, X_test_linear, y_train_linear, y_test_linear = train_test_split(X, y_linear, test_size=0.3, shuffle=False)\nX_train_logistic, X_test_logistic, y_train_logistic, y_test_logistic = train_test_split(X, y_logistic, test_size=0.3, shuffle=False)\n\n\n\n\n\nTraining the models\n\nLinear model\nThe result is shown in a barplot, where height=1 means y_pred and y_train are in the same direction, otherwise it is -1.\n\n\nCode\n# Fit the linear model\nlearning_rate = 0.00001\niterations = 1000\n\nweights_linear, y_pred_linear, perform_df_linear = fit_linear(X_train_linear.values, y_train_linear.values, lr=learning_rate, num_iter=iterations)\n\nloss = my_rmse(y_train_linear.values, y_pred_linear)\ny_train_bin = normalize_and_binarize(y_train_linear.values)\ny_pred_bin = normalize_and_binarize(y_pred_linear)\naccu = accuracy_score(y_train_bin, y_pred_bin)\nprint(f\"Final loss:{loss:.2f}, accuracy: {accu:.2f}\")\n\nx_axis = X_train_linear.index\ny_signal = np.where(y_train_bin == y_pred_bin, 1, -1)\n\nfig = plt.figure()\nplt.bar(x_axis, y_signal)\nplt.xlabel(\"Date\", fontname='serif', color='darkred')\nplt.xticks(rotation=30)\nplt.ylabel(\"Prediction\\nOutcome\", fontname='serif', color='darkred')\nplt.title(\"Performance in Linear regression Training\", fontname='serif', color='darkblue', fontsize=16)\n# plt.legend()\nplt.show()\n# Evaluate the models\n# ... (depends on how you want to evaluate your models)\n\n\nFinal loss:0.03, accuracy: 0.51\n\n\n\n\n\n\n\nLogistic model\nThe result is shown in a barplot, where height=1 means y_pred and y_train are the same, otherwise it is -1.\n\n\nCode\n# Fit the logistic model\nlearning_rate = 0.01\niterations = 1000\n\nweights_logistic, h_pred_logistic, perform_df_logistic = fit_logistic(X_train_logistic.values, y_train_logistic, lr=learning_rate, num_iter=iterations)\n\ny_pred_logistic = normalize_and_binarize(h_pred_logistic)\n\nloss = logLoss(y_train_logistic, h_pred_logistic)\naccu = accuracy_score(y_train_logistic, y_pred_logistic)\nprint(f\"Final loss:{loss:.2f}, accuracy: {accu:.2f}\")\n\nx_axis = X_train_logistic.index\ny_signal = np.where(y_train_logistic == y_pred_logistic, 1, -1)\n\nfig = plt.figure()\nplt.bar(x_axis, y_signal)\nplt.xlabel(\"Date\", fontname='serif', color='darkred')\nplt.xticks(rotation=30)\nplt.ylabel(\"Prediction\\nOutcome\", fontname='serif', color='darkred')\nplt.title(\"Performance in Logistic regression Training\", fontname='serif', color='darkblue', fontsize=16)\n# plt.legend()\nplt.show()\n\n\nFinal loss:0.68, accuracy: 0.56\n\n\n\n\n\n\n\n\nTest the performance of Strategy based on the prediction\nWhen we predict the return to be negative, we want to short the stock, otherwise long it. This time we are performing the prediction on the test dataset.\n\n\nCode\ny_pred_linear = linearPredict(X_test_linear, weights_linear)\ny_pred_logistic = linearPredict(X_test_logistic, weights_logistic)\n\ntest_return_arr = y_test_linear\n\n# calculate the return\ny_pred_linear_return = np.where(y_pred_linear &gt; 0, test_return_arr, -test_return_arr)\ny_pred_logistic_return = np.where(y_pred_logistic &gt; 0, test_return_arr, -test_return_arr)\nx_axis = test_return_arr.index\n\ncum_test_return = (1+test_return_arr).cumprod()\ncum_linear_return = (1+y_pred_linear_return).cumprod()\ncum_logistic_return = (1+y_pred_logistic_return).cumprod()\n\n\nfig = plt.figure()\nplt.plot(x_axis, cum_test_return, label='test_return')\nplt.plot(x_axis, cum_linear_return, label='linear_return')\nplt.plot(x_axis, cum_logistic_return, label='logistic_return')\nplt.xlabel(\"Date\", fontname='serif', color='darkred')\nplt.xticks(rotation=30)\nplt.ylabel(\"Cumulative\\nReturn\", fontname='serif', color='darkred')\nplt.title(\"Strategy comparison on testset\", fontname='serif', color='darkblue', fontsize=16)\nplt.legend()\nplt.show()\n\n\n\n\n\nend.\nprevious go back"
  }
]