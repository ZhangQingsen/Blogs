[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cover page",
    "section": "",
    "text": "About this blog\ntry to do some interaction here later\n\n3 - Code\nThis is inline code plus a small code chunk.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndf = pd.DataFrame(data=[[1,2,3,4],\n[4,3,2,1]], columns=[\"A\",\"B\",\"C\",\"D\"],index=[\"E\", \"F\"])\n# print(df)\nplt.plot(df)\n\n\n\n\n\n\n\n\n11111\n\ndf\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\nE\n1\n2\n3\n4\n\n\nF\n4\n3\n2\n1\n\n\n\n\n\n\n\n4 - Some math stuff\n\\[\n\\int_0^1 f(x) \\ dx\n\\]"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/misc/01.Private Server Setup notes.html",
    "href": "posts/misc/01.Private Server Setup notes.html",
    "title": "01 week01",
    "section": "",
    "text": "Device\n111 ### OS 111 ### OMV (Debian 11) 111 ### Docker 111 ### Nginx && NextCloud 111\nPrint your full name here to replace “Hello World!” 打印姓名\n\nprint(\"Hello World!\")\n\nHello World!\n\n\nview/hide line numbers in jupyter notebook 显示/隐藏行数\nclick View -&gt; click Toggle Line Numbers 点击View -&gt; 点击Toggle Line Numbers\n\n# This is line #5   这是第5行\n\nthe order of executing block changes results jupyter notebook 中未运行的代码块对后面代码块无影响\n\nimport numpy as np\n\n\nprint(np.__version__)\n\n1.26.4\n\n\nthe order of executing block changes results jupyter notebook 代码运行顺序对结果有影响\n\na=5\n\n\nprint(a)\n\n5\n\n\n\na=6\n\nIf you run the last code block before the print block, the output will show that a is 6."
  },
  {
    "objectID": "posts/misc/misc.html",
    "href": "posts/misc/misc.html",
    "title": "Miscellaneous",
    "section": "",
    "text": "This series contains miscellaneous materials I have.\n\n\n\n\n\n\n\n\n\n01 week01\n\n\nweek01 class materials, simple jupyter notebook tips\n\n\n\n\n\nOct 6, 2023\n\n1 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blogs",
    "section": "",
    "text": "Series\n\nIntro to Python\n\n\nMachine Learning\n\n\nMisc\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nSep 9, 2023\n\n\nTristan O’Malley\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/machine_learning/machine_learning.html",
    "href": "posts/machine_learning/machine_learning.html",
    "title": "Blogs in Machine Learning",
    "section": "",
    "text": "About this series\nBlogs in Machine learning\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\nDDIM\n\n\n\nPython\n\nDiffusion\n\n\n\nImplement my verison of HHL step by step with a numerical example\n\n\n\n\n\nNov 20, 2025\n\n2 min\n\n\n\n\n\n\nDDPM Forward Backword and Loss\n\n\n\nPython\n\nDiffusion\n\n\n\nImplement my verison of HHL step by step with a numerical example\n\n\n\n\n\nNov 10, 2025\n\n2 min\n\n\n\n\n\n\nHHL Algorithm Implementation in Qiskit - Complete Version\n\n\n\nPython\n\nQuantum Computing\n\n\n\nImplement my verison of HHL step by step with a numerical example\n\n\n\n\n\nJan 31, 2025\n\n7 min\n\n\n\n\n\n\nHHL Algorithm Implementation in Qiskit - Uncomputation (Inverse QPE)\n\n\n\nPython\n\nQuantum Computing\n\n\n\nImplement my verison of HHL step by step with a numerical example\n\n\n\n\n\nJan 30, 2025\n\n8 min\n\n\n\n\n\n\nHHL Algorithm Implementation in Qiskit - Controlled Rotation & Measurement of the Ancilla Qubit\n\n\n\nPython\n\nQuantum Computing\n\n\n\nImplement my verison of HHL step by step with a numerical example\n\n\n\n\n\nJan 25, 2025\n\n11 min\n\n\n\n\n\n\nHHL Algorithm Implementation in Qiskit - Quantum Phase Estimation (QPE)\n\n\n\nPython\n\nQuantum Computing\n\n\n\nImplement my verison of HHL step by step with a numerical example\n\n\n\n\n\nJan 20, 2025\n\n5 min\n\n\n\n\n\n\nHHL Algorithm Implementation in Qiskit - State Preparation\n\n\n\nPython\n\nQuantum Computing\n\n\n\nImplement my verison of HHL step by step with a numerical example\n\n\n\n\n\nJan 15, 2025\n\n3 min\n\n\n\n\n\n\nPredicting Diamond Prices: A Naive Bayes Approach\n\n\n\nPython\n\nCS5805 23Fall\n\nMachine Learning\n\n\n\nThis project applies Probability and the Naive Bayes algorithm to predict diamond prices using the categorical features of the Seaborn’s diamonds dataset, demonstrating the algorithm’s efficiency in handling such data.\n\n\n\n\n\nNov 16, 2023\n\n5 min\n\n\n\n\n\n\nClustering on Brain Networks Observations\n\n\n\nPython\n\nCS5805 23Fall\n\nMachine Learning\n\n\n\nThis project applies clustering algorithms on seaborn’s brain_networks dataset, notably implementing DBSCAN from scratch and comparing results from various scikit-learn models.\n\n\n\n\n\nNov 16, 2023\n\n6 min\n\n\n\n\n\n\nHarnessing Momentum: Predictive Analysis of Stock Prices Using Linear and Nonlinear Regression I\n\n\n\nPython\n\nCS5805 23Fall\n\nMachine Learning\n\n\n\nImplement my own version of linear regression and logistic regression and construct a momentum strategy based on my models in stock market. Phrase I, model implementation\n\n\n\n\n\nNov 16, 2023\n\n9 min\n\n\n\n\n\n\nHarnessing Momentum: Predictive Analysis of Stock Prices Using Linear and Nonlinear Regression II\n\n\n\nPython\n\nCS5805 23Fall\n\nMachine Learning\n\n\n\nImplement my own version of linear regression and logistic regression and construct a time series momentum strategy based on my models in stock market. Phrase II, momentum strategy\n\n\n\n\n\nNov 16, 2023\n\n7 min\n\n\n\n\n\n\nMnist dataset classification\n\n\n\nPython\n\nCS5805 23Fall\n\nMachine Learning\n\n\n\nThis project applies a one-layer perceptron, a simple form of a neural network, to classify handwritten digits from the MNIST dataset, providing insights into image recognition and machine learning model development.\n\n\n\n\n\nNov 16, 2023\n\n4 min\n\n\n\n\n\n\nOutlier Detection in Dow Jones using Support Vector Machines\n\n\n\nPython\n\nCS5805 23Fall\n\nMachine Learning\n\n\n\nExplore the application of SVM for anomaly detection in the Dow Jones data.\n\n\n\n\n\nNov 16, 2023\n\n6 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/intro_to_python/intro_to_python.html",
    "href": "posts/intro_to_python/intro_to_python.html",
    "title": "Series: Intro to Python",
    "section": "",
    "text": "About this series\nCourse materials for [Intro to Python] in 2020 spring at Jiangxi University of Software Professional Technology The contents are showing below:\n\n\n\nSchedule\nTopic\nMaterial\nDescription\n\n\n\n\nWeek 00\nprerequisite\ninstallation\nFollow the instruction to create python virtual environment.\n\n\nWeek 01\nbasic information\njupyter notebook\nFollow the instruction to play with jupyter notebook.\n\n\nWeek 02\npython grammer\nPython grammer\nApply basic python grammer to calculate.\n\n\nWeek 03\npython grammer 2\nPython grammer 2\nClass in python, file IO in python.\n\n\nWeek 04\nnumpy\nNumpy\nIntroducing the Numpy package for calculation, also a little bit Data Structure.\n\n\nWeek 05\npandas\nPandas\nIntroducing the Numpy package.\n\n\nWeek 06\nMatplotlib 1\nMatplotlib 1\nIntroducing the Matplotlib package for ploting basic plots.\n\n\nWeek 07\nMatplotlib 2\nMatplotlib 2\nIntroducing the Matplotlib package for ploting, including 3D plot and animation.\n\n\nWeek 08\nweb crawler\nspiders\nIntroducing the requests and BeautifulSoup package for basic web crawler application.\n\n\nWeek 09\nregular expression\nregex\nBasic regular expressions using re package.\n\n\nWeek 10\nMidterm Exam\n—————————————-\n—————————————————————-\n\n\nWeek 11\nRen’py\nRen’py\nSimple sample Visual Novel Project with Ren’py instructions.\n\n\nWeek 12\nFalsk\nFalsk\nBasic web application using flask.\n\n\nWeek 13\nOpenCV\nOpenCV\nAdvaced vision applications using the OpenCV package."
  },
  {
    "objectID": "posts/machine_learning/12.DDPM_forword.html",
    "href": "posts/machine_learning/12.DDPM_forword.html",
    "title": "DDPM Forward Backword and Loss",
    "section": "",
    "text": "This document provides a one step simple implementation of the Denoising Diffusion Probabilistic Models (DDPM) algorithm.\nThis document is based on the Hugging Face blog The Annotated Diffusion Model.\nThe key of Diffusion consists of 2 processes:\n\nA fixed forward process \\(q\\) that gradually adds Gaussian noise to an image, until you end up with pure noise.\nA learned reverse denoise diffusion process \\(p_{\\theta}\\) where a neural network is trained to gradually denoise an image starting from pure noise, until you end up with an actual image.\n\n\n\n\n\nDiffusion process illustration\n\n\n\nBased on the figure, let’s define some variables here:\n- The actual image \\(x_0\\)\n\nThe defined total timesteps \\(T\\)\nOne specific timestep \\(t\\).\nThe pure noise \\(x_T\\)\n\\(q(x_t | x_{t-1})\\) is the distrubition of \\(x_t\\) given \\(x_{t-1}\\)\n\\(p(x_{t-1} | x_{t})\\) is the distrubition of \\(x_{t-1}\\) given \\(x_{t}\\)\n\n\n\nAssume we have \\(q(x_0)\\) is the distribution for real data, we get \\(x_0\\) could be sampled from \\(q(x_0)\\), denoted as \\(x_0 \\sim q(x_0)\\).\nThe forward process \\(q(x_t | x_{t-1})\\) gradually adds Gaussian noise at each timestep \\(t\\).\nWe can write: \\[  \nq(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t\\mathbf{I})\n\\]\nWhere \\(\\beta_t\\) is the variance schedule. ### 2. Backword Process ### 3. Loss\n\n\nCode\n# Import necessary libraries\n\n\n\n\n\n\n\nCode\n# forward\n\n\n\n\n\n\n\nCode\n# backword\n\n\ngo back"
  },
  {
    "objectID": "posts/machine_learning/12.DDPM_forword.html#introduction",
    "href": "posts/machine_learning/12.DDPM_forword.html#introduction",
    "title": "DDPM Forward Backword and Loss",
    "section": "",
    "text": "This document provides a one step simple implementation of the Denoising Diffusion Probabilistic Models (DDPM) algorithm.\nThis document is based on the Hugging Face blog The Annotated Diffusion Model.\nThe key of Diffusion consists of 2 processes:\n\nA fixed forward process \\(q\\) that gradually adds Gaussian noise to an image, until you end up with pure noise.\nA learned reverse denoise diffusion process \\(p_{\\theta}\\) where a neural network is trained to gradually denoise an image starting from pure noise, until you end up with an actual image.\n\n\n\n\n\nDiffusion process illustration\n\n\n\nBased on the figure, let’s define some variables here:\n- The actual image \\(x_0\\)\n\nThe defined total timesteps \\(T\\)\nOne specific timestep \\(t\\).\nThe pure noise \\(x_T\\)\n\\(q(x_t | x_{t-1})\\) is the distrubition of \\(x_t\\) given \\(x_{t-1}\\)\n\\(p(x_{t-1} | x_{t})\\) is the distrubition of \\(x_{t-1}\\) given \\(x_{t}\\)\n\n\n\nAssume we have \\(q(x_0)\\) is the distribution for real data, we get \\(x_0\\) could be sampled from \\(q(x_0)\\), denoted as \\(x_0 \\sim q(x_0)\\).\nThe forward process \\(q(x_t | x_{t-1})\\) gradually adds Gaussian noise at each timestep \\(t\\).\nWe can write: \\[  \nq(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t\\mathbf{I})\n\\]\nWhere \\(\\beta_t\\) is the variance schedule. ### 2. Backword Process ### 3. Loss\n\n\nCode\n# Import necessary libraries\n\n\n\n\n\n\n\nCode\n# forward\n\n\n\n\n\n\n\nCode\n# backword\n\n\ngo back"
  },
  {
    "objectID": "posts/machine_learning/12.DDPM.html",
    "href": "posts/machine_learning/12.DDPM.html",
    "title": "DDPM Forward Backword and Loss",
    "section": "",
    "text": "This document provides a one step simple implementation of the Denoising Diffusion Probabilistic Models (DDPM) algorithm.\nThis document is based on the Hugging Face blog The Annotated Diffusion Model.\nThe key of Diffusion consists of 2 processes:\n\nA fixed forward process \\(q\\) that gradually adds Gaussian noise to an image, until you end up with pure noise.\nA learned reverse denoise diffusion process \\(p_{\\theta}\\) where a neural network is trained to gradually denoise an image starting from pure noise, until you end up with an actual image.\n\n\n\n\n\nDiffusion process illustration\n\n\n\nBased on the figure, let’s define some variables here:\n- The actual image \\(x_0\\)\n\nThe defined total timesteps \\(T\\)\nOne specific timestep \\(t\\).\nThe pure noise \\(x_T\\)\n\\(q(x_t | x_{t-1})\\) is the distrubition of \\(x_t\\) given \\(x_{t-1}\\)\n\\(p(x_{t-1} | x_{t})\\) is the distrubition of \\(x_{t-1}\\) given \\(x_{t}\\)\n\n\n\nAssume we have \\(q(x_0)\\) is the distribution for real data, we get \\(x_0\\) could be sampled from \\(q(x_0)\\), denoted as \\(x_0 \\sim q(x_0)\\).\nThe forward process \\(q(x_t | x_{t-1})\\) gradually adds Gaussian noise at each timestep \\(t\\).\nWe can write: \\[  \nq(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t\\mathbf{I})\n\\]\nWhere \\(\\beta_t\\) is the variance schedule.\n\n\n\n\\[  \np_{\\theta}(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma(x_t, t))\n\\]\n\n\n\nvariational lower bound (also called ELBO) \\[  \nq(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}X_0, (1-\\bar{\\alpha_t})I)\n\\]\n\n\nCode\n# Import necessary libraries\n\n\n\n\n\n\n\nCode\n# forward\n\n\n\n\n\n\n\nCode\n# backword\n\n\nnext DDIM go back"
  },
  {
    "objectID": "posts/machine_learning/12.DDPM.html#introduction",
    "href": "posts/machine_learning/12.DDPM.html#introduction",
    "title": "DDPM Forward Backword and Loss",
    "section": "",
    "text": "This document provides a one step simple implementation of the Denoising Diffusion Probabilistic Models (DDPM) algorithm.\nThis document is based on the Hugging Face blog The Annotated Diffusion Model.\nThe key of Diffusion consists of 2 processes:\n\nA fixed forward process \\(q\\) that gradually adds Gaussian noise to an image, until you end up with pure noise.\nA learned reverse denoise diffusion process \\(p_{\\theta}\\) where a neural network is trained to gradually denoise an image starting from pure noise, until you end up with an actual image.\n\n\n\n\n\nDiffusion process illustration\n\n\n\nBased on the figure, let’s define some variables here:\n- The actual image \\(x_0\\)\n\nThe defined total timesteps \\(T\\)\nOne specific timestep \\(t\\).\nThe pure noise \\(x_T\\)\n\\(q(x_t | x_{t-1})\\) is the distrubition of \\(x_t\\) given \\(x_{t-1}\\)\n\\(p(x_{t-1} | x_{t})\\) is the distrubition of \\(x_{t-1}\\) given \\(x_{t}\\)\n\n\n\nAssume we have \\(q(x_0)\\) is the distribution for real data, we get \\(x_0\\) could be sampled from \\(q(x_0)\\), denoted as \\(x_0 \\sim q(x_0)\\).\nThe forward process \\(q(x_t | x_{t-1})\\) gradually adds Gaussian noise at each timestep \\(t\\).\nWe can write: \\[  \nq(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t\\mathbf{I})\n\\]\nWhere \\(\\beta_t\\) is the variance schedule.\n\n\n\n\\[  \np_{\\theta}(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma(x_t, t))\n\\]\n\n\n\nvariational lower bound (also called ELBO) \\[  \nq(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}X_0, (1-\\bar{\\alpha_t})I)\n\\]\n\n\nCode\n# Import necessary libraries\n\n\n\n\n\n\n\nCode\n# forward\n\n\n\n\n\n\n\nCode\n# backword\n\n\nnext DDIM go back"
  },
  {
    "objectID": "posts/machine_learning/13.DDIM.html",
    "href": "posts/machine_learning/13.DDIM.html",
    "title": "DDIM",
    "section": "",
    "text": "This document provides a one step simple implementation of the Denoising Diffusion Implicit Models (DDPM) algorithm.\nThis document is based on the paper Denoising Diffusion Implicit Models.\nThe key of Diffusion consists of 2 processes:\n\nA fixed forward process \\(q\\) that gradually adds Gaussian noise to an image, until you end up with pure noise.\nA learned reverse denoise diffusion process \\(p_{\\theta}\\) where a neural network is trained to gradually denoise an image starting from pure noise, until you end up with an actual image.\n\n\n\n\n\nDiffusion process illustration\n\n\n\nBased on the figure, let’s define some variables here:\n- The actual image \\(x_0\\)\n\nThe defined total timesteps \\(T\\)\nOne specific timestep \\(t\\).\nThe pure noise \\(x_T\\)\n\\(q(x_t | x_{t-1})\\) is the distrubition of \\(x_t\\) given \\(x_{t-1}\\)\n\\(p(x_{t-1} | x_{t})\\) is the distrubition of \\(x_{t-1}\\) given \\(x_{t}\\)\n\n\n\nAssume we have \\(q(x_0)\\) is the distribution for real data, we get \\(x_0\\) could be sampled from \\(q(x_0)\\), denoted as \\(x_0 \\sim q(x_0)\\).\nThe forward process \\(q(x_t | x_{t-1})\\) gradually adds Gaussian noise at each timestep \\(t\\).\nWe can write: \\[  \nq(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t\\mathbf{I})\n\\]\nWhere \\(\\beta_t\\) is the variance schedule.\n\n\n\n\n\n\n\n\nCode\n# Import necessary libraries\n\n\n\n\n\n\n\nCode\n# forward\n\n\n\n\n\n\n\nCode\n# backword\n\n\nprev DDPM go back"
  },
  {
    "objectID": "posts/machine_learning/13.DDIM.html#introduction",
    "href": "posts/machine_learning/13.DDIM.html#introduction",
    "title": "DDIM",
    "section": "",
    "text": "This document provides a one step simple implementation of the Denoising Diffusion Implicit Models (DDPM) algorithm.\nThis document is based on the paper Denoising Diffusion Implicit Models.\nThe key of Diffusion consists of 2 processes:\n\nA fixed forward process \\(q\\) that gradually adds Gaussian noise to an image, until you end up with pure noise.\nA learned reverse denoise diffusion process \\(p_{\\theta}\\) where a neural network is trained to gradually denoise an image starting from pure noise, until you end up with an actual image.\n\n\n\n\n\nDiffusion process illustration\n\n\n\nBased on the figure, let’s define some variables here:\n- The actual image \\(x_0\\)\n\nThe defined total timesteps \\(T\\)\nOne specific timestep \\(t\\).\nThe pure noise \\(x_T\\)\n\\(q(x_t | x_{t-1})\\) is the distrubition of \\(x_t\\) given \\(x_{t-1}\\)\n\\(p(x_{t-1} | x_{t})\\) is the distrubition of \\(x_{t-1}\\) given \\(x_{t}\\)\n\n\n\nAssume we have \\(q(x_0)\\) is the distribution for real data, we get \\(x_0\\) could be sampled from \\(q(x_0)\\), denoted as \\(x_0 \\sim q(x_0)\\).\nThe forward process \\(q(x_t | x_{t-1})\\) gradually adds Gaussian noise at each timestep \\(t\\).\nWe can write: \\[  \nq(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t\\mathbf{I})\n\\]\nWhere \\(\\beta_t\\) is the variance schedule.\n\n\n\n\n\n\n\n\nCode\n# Import necessary libraries\n\n\n\n\n\n\n\nCode\n# forward\n\n\n\n\n\n\n\nCode\n# backword\n\n\nprev DDPM go back"
  }
]