---
title: 'Mnist dataset classification'
date: '2023-11-16'
categories: ['Python', 'CS5805 23Fall', 'Machine Learning', 'Classification']
description: 'Mnist dataset classification'
format: 
  html:
    code-fold: true
execute: 
  message: false
  warning: false
editor_options: 
  chunk_output_type: console
---

```{python}
#| echo: false
#| output: false

import warnings
warnings.filterwarnings("ignore")
```

#### __Intro__
In this project, we aim to apply the One-Class Support Vector Machine (SVM) algorithm for anomaly detection on the Dow Jones dataset from seaborn. The dataset comprises two columns: Date and Price, making it a suitable candidate for time-series anomaly detection. The One-Class SVM method is particularly effective for such tasks as it defines a boundary around normal data, and any data point falling outside this boundary is considered an anomaly. By leveraging this method, we hope to identify unusual patterns in the Dow Jones index that could potentially indicate significant financial events or market abnormalities. This approach could provide valuable insights for financial analysis and decision-making processes. Stay tuned as we dive into the details of implementing and optimizing the One-Class SVM model for our dataset.

#### __Necessary Packages__
```{python}
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

plt.style.use('ggplot')
# print(f"List of seaborn datasets: \n{sns.get_dataset_names()}")
```

#### __Data Process__
##### __Download Data__
```{python}
url="https://raw.githubusercontent.com/rjafari979/Information-Visualization-Data-Analytics-Dataset-/main/mnist_test.csv"
minst_df = pd.read_csv(url)
minst_df.shape
```

#### __Data Preview__
```{python}
rows = minst_df.iloc[:100].copy(deep=True)
rows.sort_values(by="label",ascending=True, inplace=True)
plt.figure(figsize=(12,12))
plt.suptitle("MNIST Data Preview", fontname='serif', color='darkblue', fontsize=16)
for i in range(100):
  row = rows.iloc[i]
  pic = row[1:].values.reshape(28,28)
  plt.subplot(10,10,i+1)
  plt.imshow(pic)
plt.show()
```

#### __z-score normalization__
```{python}
X = minst_df.drop(columns=['label']).values
y = minst_df['label'].values

scaler = StandardScaler()
X = scaler.fit_transform(X)
```

Images after normalization
```{python}
x1 = pd.DataFrame(X)
rows = x1.iloc[:10].copy(deep=True)
# rows.sort_values(by="label",ascending=True, inplace=True)
plt.figure(figsize=(12,2))
plt.suptitle("MNIST Data after Nromalization", fontname='serif', color='darkblue', fontsize=16)
for i in range(10):
  row = rows.iloc[i]
  pic = row.values.reshape(28,28)
  plt.subplot(1,10,i+1)
  plt.imshow(pic)
plt.show()
```

### __Model Defination__
- __Sigmoid Function:__ <font color=#e8b004>$$ \sigma(x) = \frac{1}{1 + e^{-x}} $$</font>  
- __Sigmoid Derivative:__ <font color=#e8b004>$$ \sigma'(x) = \sigma(x) \cdot (1 - \sigma(x)) $$</font>  
- __Forward Propagation:__ <font color=#e8b004>$$ y_{\text{hat}} = \sigma(XW + b) $$</font>  
- __Back Propagation:__ <font color=#e8b004>$$ W_{\text{l}+1} = W_{\text{l}} + \alpha \cdot X^T \cdot ((y - y_{\text{hat}}) \cdot \sigma'(y_{\text{hat}})) $$</font>  

#### __Model Structure__
- Input Layer: 784 neurons (corresponding to the 28*28 pixel values)
- Output Layer: 1 neuron (corresponding to the predicted label)
- Activation Function: Sigmoid

```{python}
def sigmoid(x):
  x = np.clip(x, -500, 500)
  return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
  return x * (1 - x)

def compute_loss(y, y_hat):
  return np.mean((y - y_hat)**2)

def forward_propagation(X, weights, biases):
  z = np.dot(X, weights) + biases
  return sigmoid(z)

def back_propagation(X, y, y_hat, weights, learning_rate):
  error = y - y_hat
  d_weights = np.dot(X.T, error * sigmoid_derivative(y_hat))
  d_weights = np.mean(d_weights, axis=1, keepdims=True)
  weights += learning_rate * d_weights
  return weights

def fit(X, y, epochs, learning_rate):
  weights = np.random.rand(X.shape[1], 1)
  biases = np.zeros((1,))

  for epoch in range(epochs):
    y_hat = forward_propagation(X, weights, biases)
    loss = compute_loss(y, y_hat)
    weights = back_propagation(X, y, y_hat, weights, learning_rate)

    if epoch % 100 == 0:
      print(f"Epoch {epoch}, Loss: {loss:.2f}")

  return weights, biases

def predict(X, weights, biases):
    return forward_propagation(X, weights, biases)

```

#### __Prediction__
```{python}
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
weights, biases = fit(X_train, y_train, epochs=1000, learning_rate=0.01)
y_train_pred = predict(X_train, weights, biases)
y_test_pred = predict(X_test, weights, biases)

train_accuracy = accuracy_score(y_train, np.round(y_train_pred.flatten()).astype(np.int8))
test_accuracy = accuracy_score(y_test, np.round(y_test_pred.flatten()).astype(np.int8))

print(f'Train Accuracy: {train_accuracy:.2f}')
print(f'Test Accuracy: {test_accuracy:.2f}')
```

#### Predicted Labels in train dataset
```{python}
x_train1 = pd.DataFrame(X_train)
rows = x_train1.iloc[:10].copy(deep=True)
plt.figure(figsize=(12,2))
plt.suptitle("Predicted Labels in train dataset", fontname='serif', color='darkblue', fontsize=16)
for i in range(10):
  row = rows.iloc[i]
  pic = row.values.reshape(28,28)
  plt.subplot(1,10,i+1)
  plt.xlabel(f"true:{y_train[i]}\npred:{np.round(y_train_pred.flatten()[i]).astype(np.int8)}")
  plt.imshow(pic)
plt.show()
```

#### Predicted Labels in test dataset
```{python}
x_test1 = pd.DataFrame(X_test)
rows = x_test1.iloc[:10].copy(deep=True)
plt.figure(figsize=(12,2))
plt.suptitle("Predicted Labels in test dataset", fontname='serif', color='darkblue', fontsize=16)
for i in range(10):
  row = rows.iloc[i]
  pic = row.values.reshape(28,28)
  plt.subplot(1,10,i+1)
  plt.xlabel(f"true:{y_test[i]}\npred:{np.round(y_test_pred.flatten()[i]).astype(np.int8)}")
  plt.imshow(pic)
plt.show()
```

end.  
[go back](./machine_learning.qmd) 