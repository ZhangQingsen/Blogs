---
title: 'Mnist dataset classification'
date: '2023-11-16'
categories: ['Python', 'CS5805 23Fall', 'Machine Learning', 'Classification', 'Perceptron', 'Nerual Network']
description: 'This project applies a one-layer perceptron, a simple form of a neural network, to classify handwritten digits from the MNIST dataset, providing insights into image recognition and machine learning model development.'
format: 
  html:
    code-fold: true
execute: 
  message: false
  warning: false
editor_options: 
  chunk_output_type: console
---

```{python}
#| echo: false
#| output: false

import warnings
warnings.filterwarnings("ignore")
```

#### __Intro__
In this project, we aim to apply a one-layer perceptron, a simple form of a neural network, for digit recognition on the MNIST dataset. The dataset comprises 785 columns, where the first column is the label (0-9), and the rest represent the grayscale color of the corresponding pixel. The one-layer perceptron uses a softmax activation function and cross-entropy loss function. By leveraging this method, we hope to accurately classify handwritten digits from 0 to 9. This approach could provide valuable insights for image recognition tasks and machine learning model development. Stay tuned as we dive into the details of implementing and optimizing our one-layer perceptron model for the MNIST dataset.

#### __Necessary Packages__
```{python}
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

plt.style.use('ggplot')
# print(f"List of seaborn datasets: \n{sns.get_dataset_names()}")
```

#### __Data Process__
##### __Download Data__
```{python}
url="https://raw.githubusercontent.com/rjafari979/Information-Visualization-Data-Analytics-Dataset-/main/mnist_test.csv"
minst_df = pd.read_csv(url)
minst_df.shape
```

#### __Data Preview__
```{python}
rows = minst_df.iloc[:100].copy(deep=True)
rows.sort_values(by="label",ascending=True, inplace=True)
plt.figure(figsize=(12,12))
plt.suptitle("MNIST Data Preview", fontname='serif', color='darkblue', fontsize=16)
for i in range(100):
  row = rows.iloc[i]
  pic = row[1:].values.reshape(28,28)
  plt.subplot(10,10,i+1)
  plt.imshow(pic)
plt.show()
```

#### __Normalization & One-Hot Encoder__
```{python}
X = minst_df.drop(columns=['label']).values
y = minst_df['label'].values

# scaler = StandardScaler()
scaler = MinMaxScaler()
X = scaler.fit_transform(X)


encoder = OneHotEncoder(sparse=False)
y = encoder.fit_transform(y.reshape(-1, 1))
```

Images after normalization
```{python}
x1 = pd.DataFrame(X)
rows = x1.iloc[:10].copy(deep=True)
# rows.sort_values(by="label",ascending=True, inplace=True)
plt.figure(figsize=(12,2))
plt.suptitle("MNIST Data after Nromalization", fontname='serif', color='darkblue', fontsize=16)
for i in range(10):
  row = rows.iloc[i]
  pic = row.values.reshape(28,28)
  plt.subplot(1,10,i+1)
  plt.imshow(pic)
plt.show()
```

### __Model Defination__
- __Softmax Function:__  for the output layer <font color=#e8b004>$$ softmax(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}} $$
</font>  
- __Cross-entropy Loss:__ for optimization <font color=#e8b004>$$ L(y, \hat{y}) = -\sum_{i=1}^{n} y_i \log(\hat{y_i}) $$</font>  
- __Forward Propagation:__ <font color=#e8b004>$$ \hat{y} = softmax(XW + b) $$</font>  
- __Back Propagation:__ <font color=#e8b004>$$ W_{\text{l}+1} = W_{\text{l}} - \alpha \cdot X^T \cdot (y - \hat{y}) $$</font>  

#### __Model Structure__
- Input Layer: 784 neurons (corresponding to the 28*28 pixel values)
- Output Layer: 1 neuron (corresponding to the predicted label)
- Activation Function: Sigmoid

```{python}
def softmax(x):
  e_x = np.exp(x - np.max(x))
  return e_x / (e_x.sum(axis=0) + 1e-10)

def cross_entropy(y, y_hat):
  m = y.shape[0]
  y_hat = np.clip(y_hat, 1e-10, 1 - 1e-10)
  log_likelihood = -np.log(y_hat[range(m), y])
  loss = np.sum(log_likelihood) / m
  return loss

# update weight
def back_propagation(X, y, y_hat, weights, learning_rate):
  error = y_hat
  error[range(y.shape[0]), y] -= 1
  d_weights = np.dot(X.T, error)
  weights -= learning_rate * d_weights
  return weights

# predict y_hat
def forward_propagation(X, weights, biases):
  z = np.dot(X, weights) + biases
  return softmax(z)

def fit(X, y, epochs, learning_rate):
  weights = np.random.rand(X.shape[1], 10) * 0.01
  biases = np.zeros((10,))

  for epoch in range(epochs):
    y_hat = forward_propagation(X, weights, biases)
    loss = cross_entropy(y, y_hat)
    weights = back_propagation(X, y, y_hat, weights, learning_rate)

    if epoch % 100 == 0:
      print(f"Epoch {epoch}, Loss: {loss:.2f}")
      # print(f"Weights: {weights}")
      # print(f"Predictions: {y_hat}")

  return weights, biases

def predict(X, weights, biases):
  return np.argmax(forward_propagation(X, weights, biases), axis=1)

```

#### __Prediction__
```{python}
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

weights, biases = fit(X_train, np.argmax(y_train, axis=1), epochs=500, learning_rate=0.00001)

y_train_pred = predict(X_train, weights, biases)
y_test_pred = predict(X_test, weights, biases)

train_accuracy = accuracy_score(np.argmax(y_train, axis=1), y_train_pred)
test_accuracy = accuracy_score(np.argmax(y_test, axis=1), y_test_pred)

print(f'Train Accuracy: {train_accuracy:.2f}')
print(f'Test Accuracy: {test_accuracy:.2f}')
```

#### Predicted Labels in train dataset
```{python}
x_train1 = pd.DataFrame(X_train)
rows = x_train1.iloc[:10].copy(deep=True)
plt.figure(figsize=(12,2))
plt.suptitle("Predicted Labels in train dataset", fontname='serif', color='darkblue', fontsize=16)
for i in range(10):
  row = rows.iloc[i]
  pic = row.values.reshape(28,28)
  plt.subplot(1,10,i+1)
  plt.xlabel(f"True: {np.argmax(y_train[i])}\nPred: {y_train_pred[i]}")
  plt.imshow(pic)
plt.show()
```

#### Predicted Labels in test dataset
```{python}
x_test1 = pd.DataFrame(X_test)
rows = x_test1.iloc[:10].copy(deep=True)
plt.figure(figsize=(12,2))
plt.suptitle("Predicted Labels in test dataset", fontname='serif', color='darkblue', fontsize=16)
for i in range(10):
  row = rows.iloc[i]
  pic = row.values.reshape(28,28)
  plt.subplot(1,10,i+1)
  plt.xlabel(f"True: {np.argmax(y_test[i])}\nPred: {y_test_pred[i]}")
  plt.imshow(pic)
plt.show()
```

end.  
[go back](./machine_learning.qmd) 